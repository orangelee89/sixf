--- git status ---
On branch test1
Your branch and 'origin/test1' have diverged,
and have 1 and 1 different commits each, respectively.
  (use "git pull" to merge the remote branch into yours)

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/agents/rsl_rl_ppo_cfg.py
	deleted:    sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/hexapod_balance_env_cfg.py
	deleted:    sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy 2.py
	modified:   sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy.py
	modified:   sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg copy.py
	modified:   sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	sixfeet_src/ckpt/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/agents/rsl_rl_ppo_cfg.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/agents/rsl_rl_ppo_cfg.py
index fde1b31..2888d79 100644
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/agents/rsl_rl_ppo_cfg.py
+++ b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/agents/rsl_rl_ppo_cfg.py
@@ -19,10 +19,10 @@ class PPORunnerCfg(RslRlOnPolicyRunnerCfg):
     empirical_normalization = False
     policy = RslRlPpoActorCriticCfg(
         init_noise_std=1.0,
-        # actor_hidden_dims=[128, 128, 128],
-        # critic_hidden_dims=[128, 128, 128],
-        actor_hidden_dims=[32, 32],
-        critic_hidden_dims=[32, 32],
+        actor_hidden_dims=[128, 128, 128],
+        critic_hidden_dims=[128, 128, 128],
+        # actor_hidden_dims=[32, 32],
+        # critic_hidden_dims=[32, 32],
         activation="elu",
     )
     algorithm = RslRlPpoAlgorithmCfg(
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/hexapod_balance_env_cfg.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/hexapod_balance_env_cfg.py
deleted file mode 100644
index e9d0579..0000000
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/hexapod_balance_env_cfg.py
+++ /dev/null
@@ -1,138 +0,0 @@
-# SPDX-License-Identifier: BSD-3-Clause
-#
-# Hexapod self-righting task — Manager-Based RL workflow.
-# Drop this file in: isaaclab_tasks/manager_based/hexapod_balance/
-
-import math
-import torch
-import isaaclab.sim as sim_utils
-from isaaclab.assets import ArticulationCfg, AssetBaseCfg
-from isaaclab.envs import ManagerBasedRLEnvCfg
-from isaaclab.managers import (
-    EventTermCfg as EventTerm,
-    RewardTermCfg as RewTerm,
-    TerminationTermCfg as DoneTerm,
-    SceneEntityCfg,
-)
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.math import quat_rotate
-import torch
-from isaaclab.utils.math import quat_rotate 
-
-# ---------- Helper reward / penalty functions ---------- #
-
-WORLD_UP = torch.tensor([0.0, 0.0, 1.0])
-
-def reward_upright(asset, env_ids, params=None):
-    body_up = quat_rotate(asset.data.root_quat[env_ids], WORLD_UP.to(asset.device))
-    sin_theta = torch.linalg.norm(torch.cross(body_up, WORLD_UP.to(asset.device)), dim=-1)
-    return torch.exp(-sin_theta / 0.05)
-
-def penalty_root_ang_vel(asset, env_ids, params=None):
-    ang_vel = asset.data.root_ang_vel[env_ids]
-    return torch.linalg.norm(ang_vel, dim=-1)
-
-# torque 惩罚无需改动
-
-
-def penalty_joint_torque(asset, env_ids, params=None):
-    """全部关节扭矩平方和"""
-    torque = asset.data.applied_torque[env_ids]
-    return (torque ** 2).sum(-1)
-
-# ---------- Asset ---------- #
-
-HEXAPOD_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        usd_path="/home/lee/EE_ws/src/robot_urdf/urdf/hexapod_2/hexapod.usd",
-        # 如需额外物理属性覆盖，取消下行注释：
-        # articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-        #     enabled_self_collisions=False
-        # ),
-    ),
-)
-
-# ---------- Scene ---------- #
-
-@configclass
-class HexapodSceneCfg(InteractiveSceneCfg):
-    """地面 + 六足机器人"""
-    ground = AssetBaseCfg(
-        prim_path="/World/ground",
-        spawn=sim_utils.GroundPlaneCfg(size=(20.0, 20.0)),
-    )
-    robot: ArticulationCfg = HEXAPOD_CFG.replace(
-        prim_path="{ENV_REGEX_NS}/Robot"   # 每个并行 env 独立路径
-    )
-
-# ---------- Random-orientation reset ---------- #
-
-def reset_root_with_random_orientation(asset, env_ids, params=None):
-    roll  = (torch.rand(len(env_ids), device=asset.device) - 0.5) * 2 * math.pi
-    pitch = (torch.rand(len(env_ids), device=asset.device) - 0.5) * 2 * math.pi
-    yaw   = (torch.rand(len(env_ids), device=asset.device) - 0.5) * 2 * math.pi
-    quat = torch.stack([
-        torch.cos(roll/2)*torch.cos(pitch/2)*torch.cos(yaw/2) +
-        torch.sin(roll/2)*torch.sin(pitch/2)*torch.sin(yaw/2),
-
-        torch.sin(roll/2)*torch.cos(pitch/2)*torch.cos(yaw/2) -
-        torch.cos(roll/2)*torch.sin(pitch/2)*torch.sin(yaw/2),
-
-        torch.cos(roll/2)*torch.sin(pitch/2)*torch.cos(yaw/2) +
-        torch.sin(roll/2)*torch.cos(pitch/2)*torch.sin(yaw/2),
-
-        torch.cos(roll/2)*torch.cos(pitch/2)*torch.sin(yaw/2) -
-        torch.sin(roll/2)*torch.sin(pitch/2)*torch.cos(yaw/2),
-    ], dim=-1)
-    asset._default_root_state[env_ids, 3:7] = quat
-
-# ---------- Events ---------- #
-
-@configclass
-class EventCfg:
-    random_orientation = EventTerm(
-        func=reset_root_with_random_orientation,
-        mode="reset",
-        params={"asset_cfg": SceneEntityCfg("robot")},
-    )
-
-# ---------- Rewards ---------- #
-
-@configclass
-class RewardsCfg:
-    upright = RewTerm(
-        func=reward_upright, weight=5.0,
-        params={"asset_cfg": SceneEntityCfg("robot")},
-    )
-    ang_vel = RewTerm(
-        func=penalty_root_ang_vel, weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot")},
-    )
-    torque  = RewTerm(
-        func=penalty_joint_torque, weight=-2e-4,
-        params={"asset_cfg": SceneEntityCfg("robot")},
-    )
-
-# ---------- Terminations ---------- #
-
-@configclass
-class TerminationsCfg:
-    time_out = DoneTerm(func=lambda *a, **kw: False, time_out=True)  # 仅超时结束
-
-# ---------- Environment master config ---------- #
-
-@configclass
-class HexapodBalanceEnvCfg(ManagerBasedRLEnvCfg):
-    """六足自扶正（Manager-Based 工作流）"""
-    scene: HexapodSceneCfg = HexapodSceneCfg(num_envs=4096, env_spacing=4.0)
-    actions = ManagerBasedRLEnvCfg.actions   # 默认：全 DOF 位置控制
-    events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    terminations: TerminationsCfg = TerminationsCfg()
-
-    def __post_init__(self):
-        self.decimation = 2          # 60 Hz 控制
-        self.episode_length_s = 6.0
-        self.sim.dt = 1 / 120
-        self.sim.render_interval = self.decimation
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy 2.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy 2.py
deleted file mode 100644
index 28390ae..0000000
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy 2.py	
+++ /dev/null
@@ -1,166 +0,0 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# SPDX-License-Identifier: BSD-3-Clause
-#
-# 六足机器人环境：保持水平 + 扶正
-
-from __future__ import annotations
-
-import torch
-from collections.abc import Sequence
-import isaaclab.sim as sim_utils
-from isaaclab.assets import Articulation
-from isaaclab.envs import DirectRLEnv
-from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
-from isaaclab.utils.math import quat_rotate
-
-from .sixfeet_env_cfg import SixfeetEnvCfg
-
-
-# ───────────────────────────────── TorchScript 奖励函数 ──────────────────────────
-@torch.jit.script
-def reward_upright(root_quat: torch.Tensor) -> torch.Tensor:
-    """
-    指标: 机体 +Z 与世界 +Z 夹角 θ
-    公式: exp(-(1-cosθ)/0.1)  → θ=0°:1.0, θ=90°:0.36, θ=180°:0.14
-    """
-    B = root_quat.shape[0]
-    world_up = root_quat.new_zeros((B, 3))
-    world_up[:, 2] = 1.0                         # (0,0,1)
-    body_up = quat_rotate(root_quat, world_up)   # (B,3)
-    z_cos = body_up[..., 2].clamp(-1.0, 1.0)     # 取对齐度
-    return torch.exp(-(1.0 - z_cos) / 0.1)       # (B,)
-
-
-@torch.jit.script
-def penalty_ang_vel(root_ang_vel: torch.Tensor) -> torch.Tensor:
-    return torch.linalg.norm(root_ang_vel, dim=-1)
-
-
-@torch.jit.script
-def penalty_torque(joint_tau: torch.Tensor) -> torch.Tensor:
-    return (joint_tau ** 2).sum(-1)
-# ────────────────────────────────────────────────────────────────────────────────
-
-
-class SixfeetEnv(DirectRLEnv):
-    """六足机器人在任意朝向下扶正并保持水平"""
-    cfg: SixfeetEnvCfg
-
-    # --------------------------------------------------------------------- #
-    # 初始化
-    # --------------------------------------------------------------------- #
-    def __init__(self, cfg: SixfeetEnvCfg, render_mode: str | None = None, **kwargs):
-        super().__init__(cfg, render_mode, **kwargs)
-
-        # 方便访问的缓存
-        self.root_quat = self.robot.data.root_quat_w(device=self.device)
-        self.root_ang_vel = self.robot.data.root_ang_vel_w(device=self.device)
-        self.joint_pos = self.robot.data.joint_pos(device=self.device)
-        self.joint_vel = self.robot.data.joint_vel(device=self.device)
-        self.joint_tau = self.robot.data.applied_torque(device=self.device)
-
-        # 读取关节限位
-        limits = self.robot.data.joint_pos_limits(device=self.device)[0]  # (18,2)
-        self.joint_lower_limits = limits[:, 0]
-        self.joint_upper_limits = limits[:, 1]
-
-        self.all_joint_ids = torch.arange(self.joint_pos.shape[1], device=self.device)
-
-    # --------------------------------------------------------------------- #
-    # 场景搭建
-    # --------------------------------------------------------------------- #
-    def _setup_scene(self):
-        # 机器人
-        self.robot = Articulation(self.cfg.robot_cfg)
-
-        # 地面
-        spawn_ground_plane("/World/ground", GroundPlaneCfg())
-
-        # 克隆 env
-        self.scene.clone_environments(copy_from_source=False)
-        self.scene.articulations["robot"] = self.robot
-
-        # 环境光
-        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
-        light_cfg.func("/World/Light", light_cfg)
-
-    # --------------------------------------------------------------------- #
-    # RL 生命周期钩子
-    # --------------------------------------------------------------------- #
-    def _pre_physics_step(self, actions: torch.Tensor) -> None:
-        self.actions = actions.clone()(device=self.device)
-
-    def _apply_action(self) -> None:
-        # 将 [-1,1] 缩放到各关节限位
-        scaled = self.actions * (self.joint_upper_limits - self.joint_lower_limits) / 2
-        scaled += (self.joint_upper_limits + self.joint_lower_limits) / 2
-        self.robot.set_joint_position_target(scaled)
-
-    def _get_observations(self) -> dict:
-        obs = torch.cat(
-            (self.root_quat,                 # 4
-             self.root_ang_vel,              # 3
-             self.joint_pos,                 # 18
-             self.joint_vel),                # 18
-            dim=-1,
-        )
-        return {"policy": obs}
-
-    def _get_rewards(self) -> torch.Tensor:
-        r_up = reward_upright(self.root_quat)
-        p_av = penalty_ang_vel(self.root_ang_vel)
-        p_tau = penalty_torque(self.joint_tau)
-        # p_col = self.robot.get_collision_penalty()
-
-        return (
-            self.cfg.rew_scale_upright * r_up
-            + self.cfg.rew_scale_angvel * (-p_av)
-            + self.cfg.rew_scale_torque * (-p_tau)
-            # + self.cfg.rew_scale_collision * p_col
-        )
-
-    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
-        time_out = self.episode_length_buf >= self.max_episode_length - 1
-        terminated = torch.zeros_like(time_out)
-        return terminated, time_out
-
-    # --------------------------------------------------------------------- #
-    # Reset 随机化
-    # --------------------------------------------------------------------- #
-    def _reset_idx(self, env_ids: Sequence[int] | None):
-        if env_ids is None:
-            env_ids = self.robot._ALL_INDICES
-        super()._reset_idx(env_ids)
-
-        # 随机 root 姿态
-        rng = self.cfg.root_orientation_range
-        rpy = (torch.rand((len(env_ids), 3), device=self.device) - 0.5) * 2 * rng
-
-        cr, sr = torch.cos(rpy[:, 0] / 2), torch.sin(rpy[:, 0] / 2)
-        cp, sp = torch.cos(rpy[:, 1] / 2), torch.sin(rpy[:, 1] / 2)
-        cy, sy = torch.cos(rpy[:, 2] / 2), torch.sin(rpy[:, 2] / 2)
-
-        quat = torch.stack(
-            (
-                cy * cp * cr + sy * sp * sr,
-                cy * cp * sr - sy * sp * cr,
-                sy * cp * sr + cy * sp * cr,
-                sy * cp * cr - cy * sp * sr,
-            ),
-            dim=-1,
-        )
-
-        default_root = self.robot.data.default_root_state[env_ids]
-        default_root[:, 3:7] = quat                   # orientation
-        default_root[:, 2] += 0.1                    # 抬高 0.1 m 防穿地
-
-        self.robot.write_root_pose_to_sim(default_root[:, :7], env_ids)
-        self.robot.write_root_velocity_to_sim(default_root[:, 7:], env_ids)
-
-        # 关节归零
-        self.robot.write_joint_state_to_sim(
-            self.robot.data.default_joint_pos[env_ids](device=self.device),
-            self.robot.data.default_joint_vel[env_ids](device=self.device),
-            None,
-            env_ids,
-        )
\ No newline at end of file
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy.py
index 1d2396f..142bcf7 100644
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy.py	
+++ b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env copy.py	
@@ -1,369 +1,629 @@
+# sixfeet_env.py (Corrected JIT Type Hints)
 from __future__ import annotations
-
-import math
 import torch
-# 初始的CUDA设备检查打印
-# print(torch.cuda.is_available())
-# print("CUDA Device Count:", torch.cuda.device_count())
-# print("Current CUDA Device:", torch.cuda.current_device())
-# print("Device Name:", torch.cuda.get_device_name(0))
-
+import math
 from collections.abc import Sequence
+from typing import List, Dict, Optional # <<--- Added Optional
 
 import isaaclab.sim as sim_utils
 from isaaclab.assets import Articulation
 from isaaclab.envs import DirectRLEnv
-from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
-# from isaaclab.utils.math import sample_uniform # 未在您的代码中使用
-
-from .sixfeet_env_cfg import SixfeetEnvCfg # 假设配置文件在同一目录下
-from isaaclab.utils.math import quat_rotate # 确保这是您Isaac Lab版本中正确的导入路径和函数
+from isaaclab.sensors import ContactSensor
+from isaaclab.utils.math import (
+    quat_rotate,
+    quat_from_angle_axis, # <<--- 确保这一行存在
+    quat_conjugate,
+    convert_quat,
+    euler_xyz_from_quat
+)
+# from isaaclab.terrains import TerrainImporter # Used via cfg
+
+from .sixfeet_env_cfg import SixfeetEnvCfg
+
+# ───────────────── 辅助函数 ──────────────────
+@torch.jit.script
+def normalize_angle_for_obs(x: torch.Tensor) -> torch.Tensor:
+    return torch.atan2(torch.sin(x), torch.cos(x))
 
 @torch.jit.script
-def reward_upright(root_quat_xyzw: torch.Tensor) -> torch.Tensor:
-    """
-    计算基于机器人Z轴与世界Z轴对齐程度的奖励。
-    输入:  root_quat_xyzw (B,4)  —— 机体四元数，顺序为 (x,y,z,w)。
-    输出:  (B,)                  —— 奖励值, exp(-(1-cos_theta)/0.1)，theta是机体+Z与世界+Z的夹角。
-    """
-    B: int = root_quat_xyzw.shape[0]
-
-    # 世界坐标系的 +Z 轴向量
-    world_z_axis = torch.zeros((B, 3), device=root_quat_xyzw.device, dtype=root_quat_xyzw.dtype)
-    world_z_axis[:, 2] = 1.0
-
-    # 机体局部坐标系的 +Z 轴向量
-    body_z_in_body_frame = torch.zeros((B, 3), device=root_quat_xyzw.device, dtype=root_quat_xyzw.dtype)
-    body_z_in_body_frame[:, 2] = 1.0
+def compute_sixfeet_rewards_directional(
+    # --- Robot states (some might be used for reward even if not in obs) ---
+    root_lin_vel_b: torch.Tensor, # 本体线速度，用于计算奖励，但不在策略观测中
+    root_ang_vel_b: torch.Tensor,
+    projected_gravity_b: torch.Tensor,
+    joint_pos_rel: torch.Tensor,
+    joint_vel: torch.Tensor,
+    applied_torque: torch.Tensor,
+    joint_acc: torch.Tensor,
+    q_lower_limits: torch.Tensor,
+    q_upper_limits: torch.Tensor,
+    current_joint_pos_abs: torch.Tensor,
+    actions_from_policy: torch.Tensor, # Raw actions from policy
+    previous_actions_from_policy: torch.Tensor,
+    root_pos_w: torch.Tensor,
+
+    # --- Sensor data ---
+    undesired_contacts_active: torch.Tensor, # Boolean
+
+    # --- Commands (discrete directional) ---
+    # commands_discrete: (num_envs, 3) -> (cmd_fwd_bkwd, cmd_left_right, cmd_turn_lr) values from {-1, 0, 1}
+    commands_discrete: torch.Tensor,
+
+    # --- Reward weights and parameters (from cfg) ---
+    cfg_cmd_profile: Dict[str, float], # Contains target speeds and probabilities
+    cfg_rew_scale_move_in_commanded_direction: float,
+    cfg_rew_scale_achieve_reference_angular_rate: float,
+    cfg_rew_scale_alive: float,
+    cfg_rew_scale_target_height: float,
+    cfg_target_height_m: float,
+    cfg_rew_scale_action_cost: float,
+    cfg_rew_scale_action_rate: float,
+    cfg_rew_scale_joint_torques: float,
+    cfg_rew_scale_joint_accel: float,
+    cfg_rew_scale_lin_vel_z_penalty: float,
+    cfg_rew_scale_ang_vel_xy_penalty: float,
+    cfg_rew_scale_flat_orientation: float,
+    cfg_rew_scale_unwanted_movement_penalty: float,
+    cfg_rew_scale_dof_at_limit: float,
+    cfg_rew_scale_toe_orientation_penalty: float,
+    cfg_toe_joint_indices: Optional[torch.Tensor], # <<--- MODIFIED: Used Optional
+    cfg_rew_scale_low_height_penalty: float,
+    cfg_min_height_penalty_threshold: float,
+    cfg_rew_scale_undesired_contact: float,
+    dt: float
+) -> tuple[torch.Tensor, Dict[str, torch.Tensor]]: # <<--- MODIFIED: Return Dict of Tensors
+
+    # --- Command Interpretation ---
+    ref_ang_rate = cfg_cmd_profile["reference_angular_rate"]
+
+    # 1. Movement in Commanded Direction Reward
+    linear_vel_x_local = root_lin_vel_b[:, 0]
+    linear_vel_y_local = root_lin_vel_b[:, 1]
     
-    # 将机体的局部+Z轴旋转到世界坐标系下
-    body_z_in_world_frame = quat_rotate(root_quat_xyzw, body_z_in_body_frame)
-
-    # 计算机体Z轴（在世界系下）与世界Z轴的点积，即夹角的余弦值
-    cos_theta = body_z_in_world_frame[..., 2].clamp(-1.0, 1.0)
-
-    return torch.exp(-(1.0 - cos_theta) / 0.1)
+    reward_fwd_bkwd_move = commands_discrete[:, 0] * linear_vel_x_local
+    reward_left_right_move = commands_discrete[:, 1] * linear_vel_y_local
+    
+    is_linear_cmd_active = (torch.abs(commands_discrete[:, 0]) > 0.5) | (torch.abs(commands_discrete[:, 1]) > 0.5)
+    reward_linear_direction = (reward_fwd_bkwd_move + reward_left_right_move) * is_linear_cmd_active.float()
+    reward_move_in_commanded_direction = reward_linear_direction * cfg_rew_scale_move_in_commanded_direction
 
-@torch.jit.script
-def penalty_ang_vel(root_ang_vel: torch.Tensor) -> torch.Tensor:
-    """计算基于角速度大小的惩罚项。"""
-    return torch.linalg.norm(root_ang_vel, dim=-1)
+    angular_vel_z_local = root_ang_vel_b[:, 2]
+    reward_angular_direction_raw = -commands_discrete[:, 2] * angular_vel_z_local # Assume cmd +1 is R turn, omega_z + is L turn
+    
+    is_turn_cmd_active = torch.abs(commands_discrete[:, 2]) > 0.5
+    turn_rate_error = torch.abs(torch.abs(angular_vel_z_local) - ref_ang_rate)
+    reward_achieve_ref_ang_rate = torch.exp(-5.0 * turn_rate_error) * is_turn_cmd_active.float() \
+                                   * cfg_rew_scale_achieve_reference_angular_rate
+    
+    reward_turn = (reward_angular_direction_raw * is_turn_cmd_active.float() * cfg_rew_scale_move_in_commanded_direction) + \
+                  reward_achieve_ref_ang_rate
+
+    # 2. Alive and Height Rewards
+    reward_alive = torch.ones_like(commands_discrete[:,0]) * cfg_rew_scale_alive
+    current_height_z = root_pos_w[:, 2]
+    height_check = torch.clamp(current_height_z / cfg_target_height_m, max=1.1)
+    reward_target_height = height_check * cfg_rew_scale_target_height
+
+    # 3. Action Penalties
+    penalty_action_cost = torch.sum(actions_from_policy**2, dim=-1) * cfg_rew_scale_action_cost
+    penalty_action_rate = torch.sum((actions_from_policy - previous_actions_from_policy)**2, dim=-1) * cfg_rew_scale_action_rate
+
+    # 4. Efficiency Penalties
+    penalty_joint_torques = torch.sum(applied_torque**2, dim=-1) * cfg_rew_scale_joint_torques
+    penalty_joint_accel = torch.sum(joint_acc**2, dim=-1) * cfg_rew_scale_joint_accel
+
+    # 5. Stability Penalties
+    penalty_lin_vel_z = torch.square(root_lin_vel_b[:, 2]) * cfg_rew_scale_lin_vel_z_penalty
+    penalty_ang_vel_xy = torch.sum(torch.square(root_ang_vel_b[:, :2]), dim=1) * cfg_rew_scale_ang_vel_xy_penalty
+    penalty_flat_orientation = torch.sum(torch.square(projected_gravity_b[:, :2]), dim=1) * cfg_rew_scale_flat_orientation
+
+    is_stand_cmd_active = torch.all(commands_discrete == 0, dim=1)
+    unwanted_lin_vel_sq = torch.sum(torch.square(root_lin_vel_b[:, :2]), dim=1)
+    unwanted_ang_vel_sq = torch.square(root_ang_vel_b[:, 2])
+    penalty_unwanted_movement = (unwanted_lin_vel_sq + unwanted_ang_vel_sq) * \
+                                is_stand_cmd_active.float() * cfg_rew_scale_unwanted_movement_penalty
+
+    # 6. Constraint Penalties
+    dof_range = q_upper_limits - q_lower_limits
+    dof_range = torch.where(dof_range < 1e-6, torch.ones_like(dof_range), dof_range)
+    # Ensure current_joint_pos_abs is (num_envs, num_dof) and q_lower_limits is (num_dof)
+    q_lower_expanded = q_lower_limits.unsqueeze(0) if q_lower_limits.ndim == 1 else q_lower_limits
+    dof_range_expanded = dof_range.unsqueeze(0) if dof_range.ndim == 1 else dof_range
+    dof_pos_scaled_01 = (current_joint_pos_abs - q_lower_expanded) / dof_range_expanded
+    
+    near_lower_limit = torch.relu(0.05 - dof_pos_scaled_01)**2
+    near_upper_limit = torch.relu(dof_pos_scaled_01 - 0.95)**2
+    penalty_dof_at_limit = torch.sum(near_lower_limit + near_upper_limit, dim=-1) * cfg_rew_scale_dof_at_limit
+
+    penalty_toe_orientation = torch.zeros_like(commands_discrete[:,0], device=commands_discrete.device)
+    if cfg_rew_scale_toe_orientation_penalty != 0.0 and cfg_toe_joint_indices is not None:
+        # Ensure cfg_toe_joint_indices is not empty before indexing
+        if cfg_toe_joint_indices.numel() > 0:
+            toe_joint_positions = current_joint_pos_abs[:, cfg_toe_joint_indices]
+            penalty_toe_orientation = torch.sum(torch.relu(toe_joint_positions)**2, dim=-1) * cfg_rew_scale_toe_orientation_penalty
+    
+    is_too_low = (current_height_z < cfg_min_height_penalty_threshold).float()
+    penalty_low_height = is_too_low * cfg_rew_scale_low_height_penalty
 
-@torch.jit.script
-def penalty_torque(joint_tau: torch.Tensor) -> torch.Tensor:
-    """计算基于关节力矩平方和的惩罚项。"""
-    return (joint_tau ** 2).sum(-1)
+    penalty_undesired_contact = undesired_contacts_active.float() * cfg_rew_scale_undesired_contact
+    
+    total_reward = (
+        reward_move_in_commanded_direction + reward_turn + reward_alive + reward_target_height
+        + (penalty_action_cost + penalty_action_rate + penalty_joint_torques + penalty_joint_accel
+        + penalty_lin_vel_z + penalty_ang_vel_xy + penalty_flat_orientation + penalty_unwanted_movement
+        + penalty_dof_at_limit + penalty_toe_orientation + penalty_low_height
+        + penalty_undesired_contact) * dt
+    )
+    
+    # Ensure all dictionary values are Tensors for Dict[str, torch.Tensor]
+    reward_terms: Dict[str, torch.Tensor] = { # Explicit type for the dict literal
+        "move_in_commanded_direction": reward_move_in_commanded_direction,
+        "turn_reward_combined": reward_turn,
+        "alive": reward_alive,
+        "target_height": reward_target_height,
+        "action_cost_penalty": penalty_action_cost * dt,
+        "action_rate_penalty": penalty_action_rate * dt,
+        "joint_torques_penalty": penalty_joint_torques * dt,
+        "joint_accel_penalty": penalty_joint_accel * dt,
+        "lin_vel_z_penalty": penalty_lin_vel_z * dt,
+        "ang_vel_xy_penalty": penalty_ang_vel_xy * dt,
+        "flat_orientation_penalty": penalty_flat_orientation * dt,
+        "unwanted_movement_penalty": penalty_unwanted_movement * dt,
+        "dof_at_limit_penalty": penalty_dof_at_limit * dt,
+        "toe_orientation_penalty": penalty_toe_orientation, # Already scaled if active
+        "low_height_penalty": penalty_low_height * dt, # This was penalty_low_height before * dt, ensure consistency. Let's assume it should be scaled by dt if it's a penalty rate.
+        "undesired_contact_penalty": penalty_undesired_contact * dt,
+    }
+    # Correction for penalty_toe_orientation, it's already scaled by its weight.
+    # If other penalties are rates, they are scaled by dt. If they are "costs per step", then no dt.
+    # Given Anymal-C scales most, let's assume it's fine. Re-evaluating:
+    # penalty_toe_orientation term already has its scale cfg_rew_scale_toe_orientation_penalty
+    # If it's a cost per step due to bad orientation, it's fine without dt.
+    # If it's a continuous penalty over time, then * dt.
+    # Let's be consistent: if the `cfg_rew_scale_` implies a per-step cost, then dt might not be needed for all.
+    # However, to match Anymal-C's style which seems to scale most penalties by dt:
+    reward_terms["toe_orientation_penalty"] = penalty_toe_orientation * dt # If applying dt scaling consistently
+
+    return total_reward, reward_terms
 
 
 class SixfeetEnv(DirectRLEnv):
     cfg: SixfeetEnvCfg
+    _contact_sensor: ContactSensor
 
     def __init__(self, cfg: SixfeetEnvCfg, render_mode: str | None = None, **kwargs):
-        # 调用父类初始化，这会间接调用 _setup_scene
         super().__init__(cfg, render_mode, **kwargs)
 
-        # 初始化可能在 _setup_scene 或 _post_reset 中才被赋值的变量
-        self.joint_lower_limits: torch.Tensor | None = None
-        self.joint_upper_limits: torch.Tensor | None = None
-        self.all_joint_ids: torch.Tensor | None = None # 如果需要所有关节的索引
-        self.target_joint_positions: torch.Tensor | None = None # 目标站立关节角度
-        self.joint_pos_target: torch.Tensor | None = None # PD控制器的目标关节角度
-
-        # 调试信息：检查碰撞报告是否在配置中启用
-        contact_reporting_enabled = False
-        if hasattr(self.cfg, 'robot_cfg') and hasattr(self.cfg.robot_cfg, 'collision_props') and self.cfg.robot_cfg.collision_props.report_contacts:
-            contact_reporting_enabled = True
-        elif hasattr(self.cfg, 'robot') and hasattr(self.cfg.robot, 'collision_props') and self.cfg.robot.collision_props.report_contacts: # Isaac Lab 0.6+ style
-            contact_reporting_enabled = True
+        self._default_joint_pos = self.robot.data.default_joint_pos.clone()
+        # Ensure _default_joint_pos is (num_dof) or (1, num_dof) for broadcasting
+        if self._default_joint_pos.ndim > 1 and self._default_joint_pos.shape[0] == self.num_envs:
+            self._default_joint_pos = self._default_joint_pos[0] # Take from first env, assume same for all defaults
         
-        if contact_reporting_enabled:
-            print("[INFO] SixfeetEnv: Contact reporting is ENABLED in robot configuration.")
-        else:
-            print("[WARNING] SixfeetEnv: Contact reporting might NOT be enabled in robot configuration. Collision penalty may not work as expected.")
+        joint_limits = self.robot.data.joint_pos_limits[0].to(self.device)
+        self._q_lower_limits = joint_limits[:, 0]
+        self._q_upper_limits = joint_limits[:, 1]
+        
+        self._policy_actions = torch.zeros(self.num_envs, self.cfg.action_space, device=self.device)
+        self._previous_policy_actions = torch.zeros_like(self._policy_actions)
+        self._processed_actions = torch.zeros_like(self._policy_actions)
 
+        self._commands = torch.zeros(self.num_envs, 3, device=self.device, dtype=torch.float)
+        self._time_since_last_command_change = torch.zeros(self.num_envs, device=self.device)
+        
+        self._resolve_toe_joint_indices()
+
+        self._undesired_contact_body_ids: Optional[List[int]] = None # Use Optional
+        if self.cfg.undesired_contact_link_names_expr:
+            indices, names = self._contact_sensor.find_bodies(self.cfg.undesired_contact_link_names_expr)
+            if indices: # Ensure indices list is not empty
+                self._undesired_contact_body_ids = indices
+                print(f"[INFO] SixfeetEnv: Undesired contact body IDs: {self._undesired_contact_body_ids} for names {names}")
+            else:
+                print(f"[WARNING] SixfeetEnv: No bodies for undesired contact expr: {self.cfg.undesired_contact_link_names_expr}")
+
+        self._base_body_id: Optional[List[int]] = None # Use Optional
+        if self.cfg.termination_base_contact and self.cfg.base_link_name:
+            indices, names = self._contact_sensor.find_bodies(self.cfg.base_link_name)
+            if indices: # Ensure indices list is not empty
+                self._base_body_id = indices
+                print(f"[INFO] SixfeetEnv: Base body ID for termination: {self._base_body_id} for names {names}")
+            else:
+                print(f"[WARNING] SixfeetEnv: No body for base contact termination: {self.cfg.base_link_name}")
+        
+        self._episode_reward_terms_sum: Dict[str, torch.Tensor] = {} # Ensure it's a Dict of Tensors
+
+    def _resolve_toe_joint_indices(self):
+        from typing import Optional # 确保 Optional 已导入
+
+        self._toe_joint_indices: Optional[torch.Tensor] = None 
+        expr_or_list = getattr(self.cfg, 'toe_joint_names_expr', None)
+        joint_names_list_for_logging = [] # 用于日志记录
+
+        if isinstance(expr_or_list, str):
+            joint_indices_list, joint_names_list_for_logging = self.robot.find_joints(expr_or_list)
+            if joint_indices_list: 
+                self._toe_joint_indices = torch.tensor(joint_indices_list, device=self.device, dtype=torch.long)
+                # print(f"[INFO] SixfeetEnv: Found toe joint indices via regex: {self._toe_joint_indices.tolist()}, names: {joint_names_list_for_logging}")
+            # else:
+            #     print(f"[WARNING] SixfeetEnv: No toe joints found using regex: '{expr_or_list}'.")
+        elif isinstance(expr_or_list, list) and all(isinstance(i, int) for i in expr_or_list):
+            if expr_or_list: 
+                temp_indices = torch.tensor(expr_or_list, device=self.device, dtype=torch.long)
+                # !! 使用 self.robot.num_dof !!
+                if torch.any(temp_indices < 0) or torch.any(temp_indices >= self.robot.num_dof): # <<--- 修改在这里
+                    print(f"[ERROR] SixfeetEnv: Invalid toe joint indices in list: {expr_or_list}. Max allowable index: {self.robot.num_dof - 1}")
+                    self._toe_joint_indices = None # 标记为无效
+                else:
+                    self._toe_joint_indices = temp_indices
+                    # 获取名称用于日志 (如果需要)
+                    # joint_names_list_for_logging = [self.robot.data.joint_names[i] for i in temp_indices.tolist()]
+        elif expr_or_list is not None:
+            print(f"[WARNING] SixfeetEnv: 'toe_joint_names_expr' ('{expr_or_list}') in cfg has invalid type: {type(expr_or_list)}. Expected str or list[int].")
+
+        # 统一的验证和日志打印
+        if self._toe_joint_indices is not None:
+            if self._toe_joint_indices.numel() == 0: 
+                print(f"[WARNING] SixfeetEnv: Resolved toe joint indices tensor is empty from '{expr_or_list}'.")
+                self._toe_joint_indices = None
+            # 再次验证索引范围，确保之前的逻辑（如果设置了None）在这里也能被正确处理
+            elif self.robot is not None and hasattr(self.robot, 'num_dof') and \
+                 (torch.any(self._toe_joint_indices < 0) or torch.any(self._toe_joint_indices >= self.robot.num_dof)):
+                print(f"[ERROR] SixfeetEnv: Invalid toe joint indices after processing: {self._toe_joint_indices.tolist()}. Max allowable index: {self.robot.num_dof - 1}")
+                self._toe_joint_indices = None
+            else:
+                # 只有在所有检查通过后才打印最终有效的索引
+                # （注意：joint_names_list_for_logging 可能只在 regex 分支被赋值）
+                print(f"[INFO] SixfeetEnv: Validated toe joint indices for penalty: {self._toe_joint_indices.tolist()}")
+        
+        if self._toe_joint_indices is None and expr_or_list is not None:
+             print(f"[WARNING] SixfeetEnv: No valid toe joint indices resolved from '{expr_or_list}'. Toe orientation penalty might not apply effectively.")
+        elif self._toe_joint_indices is None and expr_or_list is None:
+             print(f"[INFO] SixfeetEnv: 'toe_joint_names_expr' not specified. Toe orientation penalty will not be applied.")
 
     def _setup_scene(self):
-        # 机器人实例化
-        # 根据配置中是 robot_cfg 还是 robot 来获取 ArticulationCfg
-        if hasattr(self.cfg, 'robot_cfg'): # 兼容您之前的cfg命名
-            self.robot = Articulation(self.cfg.robot_cfg)
-        elif hasattr(self.cfg, 'robot'): # Isaac Lab 0.6+ 风格
-            self.robot = Articulation(self.cfg.robot)
-        else:
-            raise AttributeError("SixfeetEnvCfg is missing 'robot_cfg' or 'robot' attribute of type ArticulationCfg.")
-        
-        # 将机器人添加到场景的关节对象字典中，这对于Isaac Lab环境很重要
+        self.robot = Articulation(self.cfg.robot)
         self.scene.articulations["robot"] = self.robot
 
-        # 生成地面
-        spawn_ground_plane("/World/ground", GroundPlaneCfg())
+        self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
+        self.scene.sensors["contact_sensor"] = self._contact_sensor
+        
+        if hasattr(self.cfg, "terrain") and self.cfg.terrain is not None:
+            if hasattr(self.scene, "cfg"): # Should exist for InteractiveScene
+                self.cfg.terrain.num_envs = self.scene.cfg.num_envs
+                self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
+            
+            terrain_class_path = getattr(self.cfg.terrain, "class_type", None)
+            if isinstance(terrain_class_path, str):
+                # Dynamically import the class
+                # This part can be tricky with relative imports if class_type is not a fully qualified path.
+                # Assuming class_type is like "module.submodule.ClassName"
+                try:
+                    module_path, class_name = terrain_class_path.rsplit('.', 1)
+                    module = __import__(module_path, fromlist=[class_name])
+                    terrain_class = getattr(module, class_name)
+                except Exception as e:
+                    print(f"[ERROR] Failed to import terrain class {terrain_class_path}: {e}")
+                    from isaaclab.terrains import TerrainImporter
+                    terrain_class = TerrainImporter # Fallback
+            elif terrain_class_path is None: # Default if not specified
+                from isaaclab.terrains import TerrainImporter
+                terrain_class = TerrainImporter
+            else: # If class_type is already a class object (e.g. directly assigned in Hydra)
+                terrain_class = terrain_class_path
+
+            self._terrain = terrain_class(self.cfg.terrain) # type: ignore
+        else:
+            print("[WARNING] SixfeetEnv: No terrain configuration. Spawning default plane.")
+            from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
+            spawn_ground_plane("/World/ground", GroundPlaneCfg())
+            class DummyTerrain:
+                def __init__(self, num_envs, device): self.env_origins = torch.zeros((num_envs, 3), device=device)
+            self._terrain = DummyTerrain(self.cfg.scene.num_envs, self.device) # type: ignore
 
-        # 添加光源
         light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
         light_cfg.func("/World/Light", light_cfg)
+        self.scene.clone_environments(copy_from_source=False)
 
-        # 注意: self.robot.data 中的数据（如joint_pos_limits）在物理引擎至少运行一步或重置后才完全可靠。
-        # _post_reset 是一个更安全的地方来初始化这些依赖于运行时数据的变量。
-        # 但通常在 Articulation 对象创建后，其静态属性（如关节数量、名称）和配置的限制应该是可读的。
+    def _update_commands(self, env_ids: torch.Tensor):
+        # 使用 self.sim.dt 来获取仿真步长
+        self._time_since_last_command_change[env_ids] += self.physics_dt 
+        
+        change_command_mask = self._time_since_last_command_change[env_ids] >= self.cfg.command_profile["command_mode_duration_s"]
+        envs_to_change = env_ids[change_command_mask]
 
+        if envs_to_change.numel() > 0:
+            self._time_since_last_command_change[envs_to_change] = 0.0
+            num_to_change = envs_to_change.shape[0]
+            
+            # Sample new command modes
+            # 0: stand, 1: fwd, 2: bkwd, 3: left, 4: right, 5: turn_L, 6: turn_R
+            command_modes = torch.randint(0, self.cfg.command_profile["num_command_modes"], (num_to_change,), device=self.device)
+            
+            new_commands_for_changed_envs = torch.zeros(num_to_change, 3, device=self.device, dtype=torch.float)
+
+            # Stand still with higher probability
+            stand_mask = torch.rand(num_to_change, device=self.device) < self.cfg.command_profile["stand_still_prob"]
+            command_modes[stand_mask] = 0 # Force stand still for these
+
+            new_commands_for_changed_envs[command_modes == 1, 0] = 1.0  # Forward (X+)
+            new_commands_for_changed_envs[command_modes == 2, 0] = -1.0 # Backward (X-)
+            new_commands_for_changed_envs[command_modes == 3, 1] = -1.0 # Left Strafe (Y-)
+            new_commands_for_changed_envs[command_modes == 4, 1] = 1.0  # Right Strafe (Y+)
+            new_commands_for_changed_envs[command_modes == 5, 2] = -1.0 # Turn Left (Yaw-)
+            new_commands_for_changed_envs[command_modes == 6, 2] = 1.0  # Turn Right (Yaw+)
+            
+            self._commands[envs_to_change] = new_commands_for_changed_envs
 
     def _pre_physics_step(self, actions: torch.Tensor):
-        # 克隆动作张量并确保它在正确的设备上
-        self.actions = actions.clone().to(self.device)
-        # print(f"Actions shape: {self.actions.shape}") # 用于调试
+        self._policy_actions = actions.clone().to(self.device)
+        # _default_joint_pos is (num_dof), need to expand for broadcasting with (num_envs, num_dof) actions
+        if torch.any(torch.isnan(actions)) or torch.any(torch.isinf(actions)):
+            print(f"[WARNING] Invalid actions detected: {actions}")
+            actions = torch.zeros_like(actions)
+        cur_pos = self.robot.data.joint_pos   
+        self._processed_actions = cur_pos + self.cfg.action_scale * self._policy_actions
+
+        self._processed_actions = torch.clamp(
+        self._processed_actions, 
+        self._q_lower_limits.unsqueeze(0), 
+        self._q_upper_limits.unsqueeze(0)
+    )
+        all_env_ids = torch.arange(self.num_envs, device=self.device, dtype=torch.long)
+        self._update_commands(all_env_ids)
 
     def _apply_action(self):
-        # 确保关节限制已初始化 (通常在 _post_reset 中完成)
-        if self.joint_lower_limits is None or self.joint_upper_limits is None:
-            # 这是一个后备，理想情况下不应该在这里初始化
-            if self.robot.is_initialized and self.robot.num_articulated_joints > 0:
-                print("[Warning] SixfeetEnv._apply_action: Lazily initializing joint limits.")
-                limits = self.robot.data.joint_pos_limits.to(self.device)
-                if limits.ndim == 3 and limits.shape[0] == 1: # 处理可能的额外env维度
-                    limits = limits.squeeze(0)
-                self.joint_lower_limits = limits[:, 0]
-                self.joint_upper_limits = limits[:, 1]
-            else:
-                raise RuntimeError("Joint limits not initialized in SixfeetEnv, and robot not ready.")
+        self.robot.set_joint_position_target(self._processed_actions)
 
-        # 将动作值从 [-1, 1] 缩放到每个关节的 [lower_limit, upper_limit] 范围
-        action_midpoint = (self.joint_upper_limits + self.joint_lower_limits) / 2.0
-        action_half_range = (self.joint_upper_limits - self.joint_lower_limits) / 2.0
+    def _get_observations(self) -> dict:
+        self._previous_policy_actions = self._policy_actions.clone()
         
-        # 根据您的配置，action_scale 可能会进一步调整这个范围或作为偏移的乘数
-        # 您之前的代码是: self.target_joint_positions + actions * self.cfg.action_scale
-        # 这表示 actions 是相对于 target_joint_positions 的偏移。
-        # 如果 self.actions 直接代表目标位置（在[-1,1]标准化后），则使用下面的映射：
-        scaled_actions = self.actions * action_half_range + action_midpoint
+        # default_pos_expanded = self._default_joint_pos.unsqueeze(0)
+        # joint_pos_rel = self.robot.data.joint_pos - default_pos_expanded
+        joint_pos_rel = torch.zeros_like(self.robot.data.joint_pos)
+        obs_list = [
+            self.robot.data.projected_gravity_b,
+            self.robot.data.root_ang_vel_b,
+            self._commands,
+            normalize_angle_for_obs(joint_pos_rel),
+            self.robot.data.joint_vel,
+        ]
+        observations_tensor = torch.cat(obs_list, dim=-1)
         
-        # 如果 self.actions 是相对于目标站立姿态的偏移:
-        # if self.target_joint_positions is None: # 确保目标站立姿态已初始化
-        #     self._init_target_joint_positions()
-        # scaled_actions = self.target_joint_positions + self.actions * self.cfg.action_scale
-        # scaled_actions = torch.clamp(scaled_actions, self.joint_lower_limits, self.joint_upper_limits) # 裁剪
+        if hasattr(self.cfg, "observation_space") and observations_tensor.shape[1] != self.cfg.observation_space:
+            print(f"[ERROR] SixfeetEnv: Obs dim mismatch! Expected {self.cfg.observation_space}, got {observations_tensor.shape[1]}")
+            print(f"Details: proj_grav({self.robot.data.projected_gravity_b.shape}), ang_vel({self.robot.data.root_ang_vel_b.shape}), cmds({self._commands.shape}), jpos_rel({joint_pos_rel.shape}), jvel({self.robot.data.joint_vel.shape})")
 
-        self.robot.set_joint_position_target(scaled_actions)
 
-    def _get_observations(self) -> dict:
-        # 直接从 self.robot.data 获取最新的状态数据
-        root_quat_obs = self.robot.data.root_quat_w.to(self.device)         # (x,y,z,w)
-        root_ang_vel_obs = self.robot.data.root_ang_vel_w.to(self.device)
-        joint_pos_obs = self.robot.data.joint_pos.to(self.device)
-        joint_vel_obs = self.robot.data.joint_vel.to(self.device)
-        
-        obs = torch.cat(
-            (
-                root_quat_obs,       # 4
-                root_ang_vel_obs,    # 3
-                joint_pos_obs,       # num_actions (例如18)
-                joint_vel_obs,       # num_actions (例如18)
-            ),
-            dim=-1,
-        )
-        return {"policy": obs}
+        return {"policy": observations_tensor}
 
     def _get_rewards(self) -> torch.Tensor:
-        # 获取当前状态用于奖励计算
-        current_root_quat = self.robot.data.root_quat_w.to(self.device) # (x,y,z,w)
-        current_root_ang_vel = self.robot.data.root_ang_vel_w.to(self.device)
-        current_joint_tau = self.robot.data.applied_torque.to(self.device) # 注意: applied_torque 可能不总是理想的惩罚项
+        # Get necessary states (including root_lin_vel_b for reward calculation only)
+        root_lin_vel_b = self.robot.data.root_lin_vel_b # Used for reward, not obs
+        root_ang_vel_b = self.robot.data.root_ang_vel_b
+        projected_gravity_b = self.robot.data.projected_gravity_b
+        
+        # Ensure default_joint_pos is correctly shaped for broadcasting
+        if self._default_joint_pos.ndim == 1:
+             default_pos_expanded = self._default_joint_pos.unsqueeze(0)
+        else: # Should be (1, num_dof) or already (num_envs, num_dof)
+             default_pos_expanded = self._default_joint_pos
+
+        joint_pos_rel = self.robot.data.joint_pos - default_pos_expanded
+        current_joint_pos_abs = self.robot.data.joint_pos
+        
+        joint_vel = self.robot.data.joint_vel
+        applied_torque = self.robot.data.applied_torque
+        joint_acc = getattr(self.robot.data, "joint_acc", torch.zeros_like(joint_vel))
+        root_pos_w = self.robot.data.root_pos_w
+
+        # --- undesired_contacts_active 计算修改开始 ---
+        undesired_contacts_active = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)
+        if self._undesired_contact_body_ids and len(self._undesired_contact_body_ids) > 0 :
+            if hasattr(self._contact_sensor.data, 'net_forces_w_history'):
+                all_forces_history = self._contact_sensor.data.net_forces_w_history
+                # 确保历史数据有效且包含足够的body信息
+                if all_forces_history is not None and all_forces_history.ndim == 4 and all_forces_history.shape[1] > 0 and \
+                   all_forces_history.shape[2] > max(self._undesired_contact_body_ids):
+                    current_net_forces_w = all_forces_history[:, -1, :, :] # 获取当前时间步的力
+                    forces_on_undesired_bodies = current_net_forces_w[:, self._undesired_contact_body_ids, :]
+                    force_magnitudes = torch.norm(forces_on_undesired_bodies, dim=-1)
+                    undesired_contacts_active = torch.any(force_magnitudes > 1.0, dim=1)
+            # else:
+            #     # 如果需要，可以在这里添加警告或日志
+            #     # print_once("[WARNING] ContactSensor data does not have 'net_forces_w_history' for undesired contacts in _get_rewards.")
+            #     pass
+        # --- undesired_contacts_active 计算修改结束 ---
+        
+        total_reward, reward_terms_dict = compute_sixfeet_rewards_directional(
+            root_lin_vel_b, root_ang_vel_b, projected_gravity_b,
+            joint_pos_rel, joint_vel, applied_torque, joint_acc,
+            self._q_lower_limits, self._q_upper_limits, current_joint_pos_abs,
+            self._policy_actions, self._previous_policy_actions,
+            root_pos_w,
+            undesired_contacts_active,
+            self._commands,
+            self.cfg.command_profile,
+            self.cfg.rew_scale_move_in_commanded_direction,
+            self.cfg.rew_scale_achieve_reference_angular_rate,
+            self.cfg.rew_scale_alive, self.cfg.rew_scale_target_height, self.cfg.target_height_m,
+            self.cfg.rew_scale_action_cost, self.cfg.rew_scale_action_rate,
+            self.cfg.rew_scale_joint_torques, self.cfg.rew_scale_joint_accel,
+            self.cfg.rew_scale_lin_vel_z_penalty, self.cfg.rew_scale_ang_vel_xy_penalty,
+            self.cfg.rew_scale_flat_orientation, self.cfg.rew_scale_unwanted_movement_penalty,
+            self.cfg.rew_scale_dof_at_limit,
+            self.cfg.rew_scale_toe_orientation_penalty, self._toe_joint_indices,
+            self.cfg.rew_scale_low_height_penalty, self.cfg.min_height_penalty_threshold,
+            self.cfg.rew_scale_undesired_contact,
+            self.sim.cfg.dt # <<--- 使用 self.sim.cfg.dt 获取物理步长
+        )
 
-        r_up = reward_upright(current_root_quat)
-        p_av = penalty_ang_vel(current_root_ang_vel)
-        p_tau = penalty_torque(current_joint_tau) # 或者 penalty_torque(self.actions) 如果想惩罚动作幅度
+        if "log" not in self.extras or self.extras["log"] is None : self.extras["log"] = {}
+        for key, value in reward_terms_dict.items():
+            term_mean = value.mean()
+            # 确保记录的是Python标量
+            self.extras["log"][f"reward_term/{key}_step_avg"] = term_mean.item() if torch.is_tensor(term_mean) else term_mean
+            if key not in self._episode_reward_terms_sum:
+                self._episode_reward_terms_sum[key] = torch.zeros(self.num_envs, device=self.device)
+            self._episode_reward_terms_sum[key] += value.squeeze()
 
-        # --- 碰撞惩罚计算 ---
-        computed_collision_penalty_value = torch.zeros_like(r_up) # 默认惩罚为0
 
-        if hasattr(self.robot.data, "net_contact_forces") and self.robot.data.net_contact_forces is not None:
-            contact_forces = self.robot.data.net_contact_forces.to(self.device) # 形状: (num_envs, num_links, 3)
-            
-            # 从配置中获取身体连杆索引和力分量阈值
-            # 您需要在 SixfeetEnvCfg.py 中定义这些！
-            body_link_indices = getattr(self.cfg, "body_link_indices_for_collision", [0]) # 示例：默认只检查连杆0
-            contact_component_threshold = getattr(self.cfg, "contact_component_threshold", 1.0) # 示例：默认阈值1N
-
-            valid_body_link_indices = [idx for idx in body_link_indices if idx < contact_forces.shape[1]]
-            if valid_body_link_indices:
-                # 获取指定身体连杆的接触力
-                selected_body_forces = contact_forces[:, valid_body_link_indices, :] # 形状: (num_envs, num_valid_body_links, 3)
-                
-                # 检查是否有任何力分量的绝对值超过阈值
-                component_exceeds_threshold = torch.abs(selected_body_forces) > contact_component_threshold # 形状: (num_envs, num_valid_body_links, 3)
-                
-                # 如果该连杆的x,y,z任何一个力分量超阈值，则为True
-                link_has_strong_force_component = torch.any(component_exceeds_threshold, dim=2) # 形状: (num_envs, num_valid_body_links)
-                
-                # 如果任何一个指定的身体连杆有显著的力分量，则认为发生碰撞
-                is_body_colliding = torch.any(link_has_strong_force_component, dim=1).float() # 形状: (num_envs,)
-                
-                computed_collision_penalty_value = is_body_colliding # 惩罚值为1.0（如果碰撞），0.0（如果不碰撞）
-            else:
-                # 仅在调试时或偶尔打印警告，避免刷屏
-                if self.episode_count % 100 == 0 and self.num_envs > 0 : 
-                    print(f"[Warning] SixfeetEnv._get_rewards: No valid body_link_indices ({body_link_indices}) for collision check. Max link index: {contact_forces.shape[1]-1 if contact_forces.shape[1]>0 else -1}")
-        else:
-            if self.episode_count % 100 == 0 and self.num_envs > 0:
-                 print("[Warning] SixfeetEnv._get_rewards: 'net_contact_forces' not found or is None in self.robot.data. Collision penalty will be zero.")
+        current_terminated, current_time_out = self._get_dones()
+        just_failed_termination = current_terminated & (~current_time_out)
         
-        # self.cfg.rew_scale_collision 应该是一个负数 (例如 -10.0)
-        total_reward = (
-            self.cfg.rew_scale_upright * r_up
-            + self.cfg.rew_scale_angvel * (-p_av)  # p_av 是正的范数，所以乘以负权重
-            + self.cfg.rew_scale_torque * (-p_tau)  # p_tau 是正的平方和，所以乘以负权重
-            + self.cfg.rew_scale_collision * computed_collision_penalty_value # computed_collision_penalty_value 是正的 "坏的程度"
+        final_reward = torch.where(
+            just_failed_termination,
+            torch.full_like(total_reward, self.cfg.rew_scale_termination),
+            total_reward
         )
-        return total_reward
+        # 确保记录的是Python标量
+        self.extras["log"]["reward/final_reward_avg"] = final_reward.mean().item() if torch.is_tensor(final_reward) else final_reward.mean()
+        return final_reward
 
     def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
         time_out = self.episode_length_buf >= self.max_episode_length - 1
+        root_pos_w = self.robot.data.root_pos_w
+        height_too_low = root_pos_w[:, 2] < self.cfg.termination_height_thresh
         
-        # 当前：只有超时会重置环境。
-        # 如果需要因为跌倒或严重碰撞而提前终止，可以在这里添加逻辑。
-        # 例如:
-        # current_root_quat = self.robot.data.root_quat_w.to(self.device)
-        # r_up_val = reward_upright(current_root_quat)
-        # fallen_threshold = getattr(self.cfg, "fallen_threshold", 0.2) # 在cfg中定义
-        # is_fallen = r_up_val < fallen_threshold
-        # terminated = torch.logical_or(time_out, is_fallen) # 如果想因为跌倒而终止
-
-        terminated = torch.zeros_like(time_out) # 总是False，所以只有time_out会触发RL框架的重置
-        return terminated, time_out # 在RL中，通常 terminated=True 也会导致环境重置
-
-    def _reset_idx(self, env_ids: Sequence[int] | None): 
-        if env_ids is None:
-            env_ids_tensor = torch.arange(self.num_envs, device=self.device)
-        elif isinstance(env_ids, Sequence) and not isinstance(env_ids, torch.Tensor):
-            env_ids_tensor = torch.tensor(list(env_ids), device=self.device, dtype=torch.long)
-        else:
-            env_ids_tensor = env_ids # 假设已经是tensor
-
-        super()._reset_idx(env_ids_tensor) # 处理父类的重置逻辑，如 episode_length_buf
-
-        num_resets = len(env_ids_tensor)
-
-        # 随机化根节点姿态
-        # cfg.root_orientation_range 应为包含3个浮点数的列表/元组，对应R,P,Y的范围
-        # 如果是单个浮点数，则应用到所有轴
-        if isinstance(self.cfg.root_orientation_range, float):
-            orientation_ranges = torch.full((3,), self.cfg.root_orientation_range, device=self.device, dtype=torch.float32)
-        elif isinstance(self.cfg.root_orientation_range, (list, tuple)) and len(self.cfg.root_orientation_range) == 3:
-            orientation_ranges = torch.tensor(self.cfg.root_orientation_range, device=self.device, dtype=torch.float32)
-        else:
-            raise ValueError("cfg.root_orientation_range must be a float or a list/tuple of 3 floats.")
-
-        rpy = (torch.rand((num_resets, 3), device=self.device, dtype=torch.float32) - 0.5) * 2.0 * orientation_ranges.unsqueeze(0)
-
-        # 欧拉角到四元数 (RPY -> w,x,y,z)
-        cr, sr = torch.cos(rpy[:, 0] / 2.0), torch.sin(rpy[:, 0] / 2.0) # Roll
-        cp, sp = torch.cos(rpy[:, 1] / 2.0), torch.sin(rpy[:, 1] / 2.0) # Pitch
-        cy, sy = torch.cos(rpy[:, 2] / 2.0), torch.sin(rpy[:, 2] / 2.0) # Yaw
-
-        qw = cy * cp * cr + sy * sp * sr
-        qx = cy * cp * sr - sy * sp * cr
-        qy = sy * cp * sr + cy * sp * cr
-        qz = sy * cp * cr - cy * sp * sr
+        projected_gravity_z = self.robot.data.projected_gravity_b[:, 2]
+        fallen_over = projected_gravity_z > self.cfg.termination_body_z_thresh
+
+        base_contact_termination = torch.zeros_like(time_out, dtype=torch.bool)
+        if self.cfg.termination_base_contact and self._base_body_id and len(self._base_body_id) > 0:
+            # --- base_contact_termination 计算修改开始 ---
+            if hasattr(self._contact_sensor.data, 'net_forces_w_history'):
+                all_forces_history = self._contact_sensor.data.net_forces_w_history
+                # 确保历史数据有效且包含足够的body信息
+                if all_forces_history is not None and all_forces_history.ndim == 4 and all_forces_history.shape[1] > 0 and \
+                   all_forces_history.shape[2] > max(self._base_body_id): # 确保 body 索引有效
+                    current_net_forces_w = all_forces_history[:, -1, :, :] # 获取当前时间步的力
+                    forces_on_base = current_net_forces_w[:, self._base_body_id, :]
+                    force_magnitudes_base = torch.norm(forces_on_base, dim=-1)
+                    base_contact_termination = torch.any(force_magnitudes_base > 1.0, dim=1)
+            # else:
+            #     # print_once("[WARNING] ContactSensor data does not have 'net_forces_w_history' for base contact in _get_dones.")
+            #     pass
+            # --- base_contact_termination 计算修改结束 ---
+            
+        terminated = height_too_low | fallen_over | base_contact_termination
+        # terminated = height_too_low | fallen_over 
+        # if torch.any(terminated):
+        #     step_idx = int(self.episode_length_buf.max()) 
+        #     print(
+        #         f"[DEBUG _get_dones]  step={step_idx} | "
+        #         f"height_too_low={height_too_low.nonzero(as_tuple=True)[0].numel()}  "
+        #         f"fallen_over={fallen_over.nonzero(as_tuple=True)[0].numel()}  "
+        #         f"base_contact={base_contact_termination.nonzero(as_tuple=True)[0].numel()}"
+        #     )
+        return terminated, time_out
+    
+    
+    def _reset_idx(self, env_ids: Sequence[int] | None):
+        # 1) 通用父类逻辑 ----------------------------------------------------
+        super()._reset_idx(env_ids)
+        # print("task reseted env_ids:", env_ids)
+        eids = torch.arange(self.num_envs, device=self.device) if env_ids is None \
+            else torch.as_tensor(env_ids, device=self.device)
+        if eids.numel() == 0:
+            return
+
+        # 2) root 位置与姿态 --------------------------------------------------
+        root_state = self.robot.data.default_root_state[eids].clone()
+
+        # 加地形起点
+        if hasattr(self._terrain, "env_origins"):
+            root_state[:, :3] += self._terrain.env_origins[eids]
+
+        # 高度 + 偏移
+        h0 = self.cfg.robot.init_state.pos[2] if self.cfg.robot.init_state.pos is not None else 0.3
+        root_state[:, 2] = h0 + self.cfg.reset_height_offset
+
+        # 随机朝向
+        yaw = (torch.rand(len(eids), device=self.device) - 0.5) * 2.0 * self.cfg.root_orientation_yaw_range
+        root_state[:, 3:7] = quat_from_angle_axis(yaw, torch.tensor([0., 0., 1.], device=self.device))
+        root_state[:, 7:] = 0.0                       # 线 / 角速度 = 0
+
+        self.robot.write_root_state_to_sim(root_state, eids)
+
+        # 3) 随机关节角 -------------------------------------------------------
+        n_dof   = self._q_lower_limits.numel()
+        rand_p  = torch.rand(len(eids), n_dof, device=self.device)
+        q_range = self._q_upper_limits - self._q_lower_limits
+        rand_q  = self._q_lower_limits + rand_p * q_range         # 均匀分布
+        # ---- 如想“围绕 init_state 抖动”可改成： ----
+        # center = self.robot.cfg.init_state.joint_pos_tensor      # (dof,)
+        # sigma  = torch.deg2rad(torch.tensor(5.0, device=self.device))
+        # noise  = torch.randn(len(eids), n_dof, device=self.device) * sigma
+        # rand_q = torch.clamp(center + noise, self._q_lower_limits, self._q_upper_limits)
+
+        zero_qv = torch.zeros_like(rand_q)
+        self.robot.write_joint_state_to_sim(rand_q, zero_qv, env_ids=eids)
+
+        # **同步 PD 目标 = 当前姿态**  ← 关键一行
+        self.robot.set_joint_position_target(rand_q, env_ids=eids)
+
+        # 4) 其余缓存 ----------------------------------------------------------
+        self._time_since_last_command_change[eids] = self.cfg.command_profile["command_mode_duration_s"]
+        self._update_commands(eids)
+
+        if self.cfg.rew_scale_action_rate < 0 and hasattr(self, "previous_actions"):
+            self.previous_actions[eids] = 0.0
+
+        # 清 episodic 累加器（如有）
+        for k in self._episode_reward_terms_sum.keys():
+            self._episode_reward_terms_sum[k][eids] = 0.0
+
+    # def _reset_idx(self, env_ids: Sequence[int] | None):
+    #     # Call parent first to handle episode_length_buf, reset_buf, etc.
+    #     super()._reset_idx(env_ids) 
+
+    #     if env_ids is None: # If None, superclass might have reset all, or we operate on all
+    #         eids = torch.arange(self.num_envs, device=self.device, dtype=torch.long)
+    #     else:
+    #         eids = torch.as_tensor(env_ids, device=self.device, dtype=torch.long)
         
-        # 转换为 Isaac Lab 通常使用的 (x,y,z,w) 顺序
-        quat_xyzw = torch.stack([qx, qy, qz, qw], dim=-1)
+    #     if eids.numel() == 0: # No environments to reset specifically by this function call
+    #         return
 
-        # 获取对应环境的默认根状态并修改
-        default_root_state_selected = self.robot.data.default_root_state[env_ids_tensor].clone() # 使用 .clone() 避免修改原始默认状态
+    #     # Reset robot state
+    #     root_state_reset = self.robot.data.default_root_state[eids].clone()
+    #     if hasattr(self._terrain, 'env_origins') and self._terrain.env_origins is not None:
+    #          root_state_reset[:, :3] += self._terrain.env_origins[eids]
         
-        # 设置新的随机姿态
-        default_root_state_selected[:, 3:7] = quat_xyzw
+    #     # Use initial height from cfg.robot.init_state.pos[2] + reset_height_offset
+    #     initial_height = self.cfg.robot.init_state.pos[2] if self.cfg.robot.init_state.pos is not None else 0.3 # Fallback height
+    #     root_state_reset[:, 2] = initial_height + self.cfg.reset_height_offset
+
+    #     random_yaw = (torch.rand(eids.shape[0], device=self.device) - 0.5) * 2.0 * self.cfg.root_orientation_yaw_range
+    #     world_z_axis = torch.tensor([0.0, 0.0, 1.0], device=self.device, dtype=random_yaw.dtype)
+    #     orientation_quat_xyzw = quat_from_angle_axis(random_yaw, world_z_axis)
+    #     root_state_reset[:, 3:7] = orientation_quat_xyzw
+    #     root_state_reset[:, 7:] = 0.0 #lin_vel, ang_vel
         
-        # 调整Z轴高度以防穿模
-        reset_height_offset = getattr(self.cfg, "reset_height_offset", 0.1) # 从cfg获取，如果不存在则默认为0.1
-        default_root_state_selected[:, 2] += reset_height_offset
+    #     self.robot.write_root_state_to_sim(root_state_reset, eids)
         
-        # 将新的根状态写入仿真
-        self.robot.write_root_state_to_sim(default_root_state_selected, env_ids=env_ids_tensor)
-
-        # 重置关节状态 (例如，到目标站立姿态并加上一些噪声)
-        # 确保 self.target_joint_positions 已经初始化 (通常在 _post_reset 中完成)
-        if self.target_joint_positions is None:
-            self._init_target_joint_positions() # 辅助函数来初始化
-
-        target_joints_for_reset = self.target_joint_positions[env_ids_tensor]
-        reset_joint_pos_noise_val = getattr(self.cfg, "reset_joint_pos_noise", 0.1) # 从cfg获取，默认为0.1
-        joint_pos_noise = (torch.rand_like(target_joints_for_reset) - 0.5) * 2.0 * reset_joint_pos_noise_val
-        reset_joint_pos = target_joints_for_reset + joint_pos_noise
-        
-        # 裁剪到关节限制 (确保 self.joint_lower_limits 和 self.joint_upper_limits 已初始化)
-        if self.joint_lower_limits is not None and self.joint_upper_limits is not None:
-             reset_joint_pos = torch.clamp(reset_joint_pos, self.joint_lower_limits.unsqueeze(0).repeat(num_resets,1), self.joint_upper_limits.unsqueeze(0).repeat(num_resets,1))
-        else: 
-            # 这个警告不应该经常出现，如果_post_reset正确工作
-            print("[Warning] SixfeetEnv._reset_idx: Joint limits not available for clamping reset positions. Attempting to initialize them now.")
-            self._init_joint_limits() # 尝试初始化
-            if self.joint_lower_limits is not None and self.joint_upper_limits is not None:
-                 reset_joint_pos = torch.clamp(reset_joint_pos, self.joint_lower_limits.unsqueeze(0).repeat(num_resets,1), self.joint_upper_limits.unsqueeze(0).repeat(num_resets,1))
-
-
-        self.robot.write_joint_state_to_sim(
-            positions=reset_joint_pos, 
-            velocities=self.robot.data.default_joint_vel[env_ids_tensor], # 通常重置为0速度
-            # torques=None, # 通常在重置时不设置力矩
-            joint_ids=None, # None 表示所有关节
-            env_ids=env_ids_tensor,
-        )
-        
-        # 初始化下一拍的PD控制器目标关节位置
-        if self.joint_pos_target is not None: # 确保 joint_pos_target 已初始化
-            self.joint_pos_target[env_ids_tensor] = reset_joint_pos[:]
-
-
-    def _init_target_joint_positions(self):
-        """辅助函数，用于初始化目标站立关节角度。"""
-        if hasattr(self.cfg, 'target_standing_joint_angles'):
-            self.target_joint_positions = torch.tensor(
-                self.cfg.target_standing_joint_angles, device=self.device, dtype=torch.float32
-            ).repeat(self.num_envs, 1) # 扩展到所有环境
-        else:
-            # 如果配置中没有定义，则回退到机器人的默认关节位置
-            if self.robot.is_initialized and self.robot.num_articulated_joints > 0:
-                self.target_joint_positions = self.robot.data.default_joint_pos.clone().to(self.device)
-                print("[Warning] SixfeetEnv._init_target_joint_positions: 'target_standing_joint_angles' not found in cfg. Using default_joint_pos as target.")
-            else:
-                # 这种情况理论上不应发生，如果发生在 _post_reset 之后
-                raise RuntimeError("Cannot initialize target_joint_positions: Robot not ready or has no joints.")
-
-    def _init_joint_limits(self):
-        """辅助函数，用于初始化关节限制。"""
-        if self.robot.is_initialized and self.robot.num_articulated_joints > 0:
-            limits = self.robot.data.joint_pos_limits.to(self.device)
-            if limits.ndim == 3 and limits.shape[0] == 1: # 处理可能的额外env维度
-                limits = limits.squeeze(0)
-            self.joint_lower_limits = limits[:, 0]
-            self.joint_upper_limits = limits[:, 1]
-            if self.all_joint_ids is None: # 如果需要，也在这里初始化
-                 self.all_joint_ids = torch.arange(self.robot.num_articulated_joints, device=self.device)
-            print("[INFO] SixfeetEnv._init_joint_limits: Joint limits initialized.")
-        else:
-            print("[Error] SixfeetEnv._init_joint_limits: Cannot initialize joint limits, robot not ready or no joints.")
-
-
-    def _post_reset(self, env_ids: torch.Tensor | None) -> None:
-        """在环境重置后调用，适合初始化依赖于运行时数据的变量。"""
-        super()._post_reset(env_ids) # 调用父类方法
-        
-        # 初始化目标站立关节角度 (如果尚未初始化)
-        if self.target_joint_positions is None:
-            self._init_target_joint_positions()
-
-        # 初始化关节限制 (如果尚未初始化)
-        if self.joint_lower_limits is None:
-            self._init_joint_limits()
-
-        # 初始化PD控制器的目标关节角度 (如果尚未初始化)
-        if self.joint_pos_target is None:
-            if self.target_joint_positions is not None:
-                self.joint_pos_target = self.target_joint_positions.clone()
-            elif self.robot.is_initialized and self.robot.num_articulated_joints > 0:
-                # 作为最后的后备，使用机器人的默认关节位置
-                self.joint_pos_target = self.robot.data.default_joint_pos.clone().to(self.device)
-                print("[Warning] SixfeetEnv._post_reset: joint_pos_target initialized to default_joint_pos as target_joint_positions was None.")
-            else:
-                # 如果机器人也未准备好，这会是一个问题
-                print("[Error] SixfeetEnv._post_reset: Cannot initialize joint_pos_target, robot not ready or target_joint_positions is None.")
+    #     default_pos_expanded = self._default_joint_pos.unsqueeze(0).expand(eids.shape[0], -1)
+    #     zero_vel = torch.zeros_like(default_pos_expanded)
+    #     self.robot.write_joint_state_to_sim(
+    #     default_pos_expanded,  # 使用默认位置，不是当前位置
+    #     zero_vel,              # 明确设置零速度
+    #     env_ids=eids
+    #     )
+
+    #     self._time_since_last_command_change[eids] = self.cfg.command_profile["command_mode_duration_s"] # Force immediate change
+    #     self._update_commands(eids)
+
+    #     if hasattr(self, '_previous_policy_actions'): self._previous_policy_actions[eids] = 0.0
+    #     if hasattr(self, '_policy_actions'): self._policy_actions[eids] = 0.0
+
+    #     # Logging episodic sums for environments that just reset
+    #     # This should ideally use the reset_buf from before it's cleared by super()._reset_idx
+    #     # For now, log the average of what was summed for the reset envs and then clear.
+    #     if "log" not in self.extras or self.extras["log"] is None : self.extras["log"] = {}
+    #     for key in list(self._episode_reward_terms_sum.keys()): # Iterate over a copy of keys
+    #         if eids.numel() > 0 and self._episode_reward_terms_sum[key].shape[0] == self.num_envs :
+    #             # This logs sum for envs that are resetting now.
+    #             # A better approach is to log sums for envs that *were* terminated in the previous step.
+    #             # This requires more careful state management of `reset_buf`.
+    #             # For simplicity, this will show the sum accumulated *until reset* for these envs.
+    #             # self.extras["log"][f"EpisodeSum/{key}_on_reset"] = self._episode_reward_terms_sum[key][eids].mean().item()
+    #             self._episode_reward_terms_sum[key][eids] = 0.0 # Clear sums for reset envs
\ No newline at end of file
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg copy.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg copy.py
index 853a19b..0bf3825 100644
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg copy.py	
+++ b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg copy.py	
@@ -1,72 +1,220 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-# SPDX-License-Identifier: BSD-3-Clause
-#
-# 说明：本文件仅整理了缩进 / 空格 / 注释排版，功能与原代码一致。
-
+# sixfeet_env_cfg.py (Simplified Commands, No root_lin_vel_b in obs)
 from __future__ import annotations
-
 import math
 
 import isaaclab.sim as sim_utils
-from isaaclab.actuators import ImplicitActuatorCfg
-from isaaclab.assets import ArticulationCfg
+from isaaclab.utils import configclass
 from isaaclab.envs import DirectRLEnvCfg
+from isaaclab.sim import SimulationCfg, PhysxCfg
+from isaaclab.assets import ArticulationCfg
+from isaaclab.actuators import ImplicitActuatorCfg
 from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sim import SimulationCfg
-from isaaclab.utils import configclass
-
+from isaaclab.terrains import TerrainImporterCfg
+from isaaclab.sensors import ContactSensorCfg
 
 @configclass
 class SixfeetEnvCfg(DirectRLEnvCfg):
-    # ─────────────────────────── 基础设置 ───────────────────────────
-    decimation: int = 2                    # 控制频率 60 Hz (=120/decimation)
-    episode_length_s: float = 6.0
+    # -------- 基本环境配置 --------
+    decimation: int = 2
+    episode_length_s: float = 20.0
+    action_space: int = 18
 
-    # 观测 / 动作空间尺寸
-    action_space: int = 18                 # 18 × 关节目标位置
-    observation_space: int = 4 + 3 + 18*2  # 4(quat)+3(ang vel)+18(pos)+18(vel)
-    state_space: int = 0                   # 不使用 “state” channel
+    # ---- 新的观测空间定义 (移除 root_lin_vel_b) ----
+    # projected_gravity_b (3) + root_ang_vel_b (3) + discrete_commands (3)
+    # + joint_pos_rel (18) + joint_vel (18)
+    # Total = 3 + 3 + 3 + 18 + 18 = 45 dimensions
+    observation_space: int = 45
 
-    # ─────────────────────────── 仿真配置 ───────────────────────────
+    state_space: int = 0
+
+    # -------- 仿真配置 --------
     sim: SimulationCfg = SimulationCfg(
-        dt=1 / 120,
+        dt=1/120,
         render_interval=decimation,
+        device="cuda:0",
+        physx=PhysxCfg(
+            enable_ccd=True,
+            solver_type=1,
+            ),
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            static_friction=0.8,
+            dynamic_friction=0.6,
+            restitution=0.0,
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply"
+        )
     )
 
-    # ─────────────────────────── 执行器统一 PD 参数 ────────────────
-    all_joints_pd = ImplicitActuatorCfg(
-    joint_names_expr=".*",          # 全部关节
-    stiffness=600.0,                # 这里随你调
-    damping=3.0                     # 这里随你调
+    # -------- 地形配置 --------
+    terrain: TerrainImporterCfg = TerrainImporterCfg(
+        prim_path="/World/ground",
+        terrain_type="plane",
+        collision_group=-1,
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply",
+            static_friction=1.0,
+            dynamic_friction=0.8,
+            restitution=0.0,
+        ),
     )
 
-    # ─────────────────────────── 机器人 USD ────────────────────────
-    robot_cfg: ArticulationCfg = ArticulationCfg(
+    # -------- 机器人配置 --------
+    # robot: ArticulationCfg = ArticulationCfg(
+    #     prim_path="/World/envs/env_.*/Robot",
+    #     spawn=sim_utils.UsdFileCfg(
+    #         usd_path="/home/lee/EE_ws/src/sixfeet_src/sixfeet/source/sixfeet/sixfeet/assets/hexapod/hexapod.usd", # !! 确保路径正确 !!
+    #         activate_contact_sensors=True,
+    #         rigid_props=sim_utils.RigidBodyPropertiesCfg(
+    #             disable_gravity=None, max_depenetration_velocity=10.0, enable_gyroscopic_forces=True
+    #         ),
+    #         articulation_props=sim_utils.ArticulationRootPropertiesCfg(
+    #             enabled_self_collisions=True, solver_position_iteration_count=12,
+    #             solver_velocity_iteration_count=1, sleep_threshold=0.005, stabilization_threshold=0.001,
+    #         ),
+    #     ),
+    #     init_state=ArticulationCfg.InitialStateCfg(
+    #         pos=(0.0, 0.0, 0.3),
+    #         # rot=(1.0, 0.0, 0.0, 0.0),
+    #         joint_pos={
+    #             "joint_11": 0.0, "joint_21": 0.0, "joint_31": 0.0, "joint_41": 0.0, "joint_51": 0.0, "joint_61": 0.0,
+    #             "joint_12": math.radians(45), "joint_22": math.radians(45), "joint_32": math.radians(45),
+    #             "joint_42": math.radians(45), "joint_52": math.radians(45), "joint_62": math.radians(45),
+    #             "joint_13": math.radians(-90), "joint_23": math.radians(-90), "joint_33": math.radians(-90),
+    #             "joint_43": math.radians(-90), "joint_53": math.radians(-90), "joint_63": math.radians(-90),
+    #         }
+    #     ),
+    #     actuators={
+    #         "all_joints": ImplicitActuatorCfg(
+    #             joint_names_expr=".*", stiffness=60.0,damping=4.0,
+    #             effort_limit_sim=6.0, velocity_limit_sim=8.0
+    #         )
+    #     }
+    # )
+    robot: ArticulationCfg = ArticulationCfg(
         prim_path="/World/envs/env_.*/Robot",
         spawn=sim_utils.UsdFileCfg(
-            usd_path="/home/lee/EE_ws/src/sixfeet_src/sixfeet/source/"
-                     "sixfeet/sixfeet/assets/hexapod_2/hexapod_2.usd",
+            usd_path="/home/lee/EE_ws/src/sixfeet_src/sixfeet/source/sixfeet/sixfeet/assets/hexapod/hexapod.usd",
+            # usd_path="/home/lee/EE_ws/src/robot_urdf/urdf/hexapod_2/hexapod_2.usd",
+            activate_contact_sensors=True,
+            rigid_props=sim_utils.RigidBodyPropertiesCfg(
+                disable_gravity=False,
+                max_depenetration_velocity=10.0,
+                enable_gyroscopic_forces=False,
+            ),
+            articulation_props=sim_utils.ArticulationRootPropertiesCfg(
+                enabled_self_collisions=True,  # <<<--- 修复：可能导致不稳定
+                solver_position_iteration_count=12,
+                solver_velocity_iteration_count=1,
+                sleep_threshold=0.005,
+                stabilization_threshold=0.001,
+            ),
         ),
-        actuators={
-            "all": all_joints_pd,
-        },
+        init_state=ArticulationCfg.InitialStateCfg(
+            pos=(0.0, 0.0, 0.15),
+            rot=(1.0, 0.0, 0.0, 0.0),  # <<<--- 修复：明确设置四元数
+            joint_pos={
+                "joint_11": 0.0, "joint_21": 0.0, "joint_31": 0.0, "joint_41": 0.0, "joint_51": 0.0, "joint_61": 0.0,
+                "joint_12": math.radians(45), "joint_22": math.radians(45), 
+                "joint_32": math.radians(45), "joint_42": math.radians(45), 
+                "joint_52": math.radians(45), "joint_62": math.radians(45),
+                "joint_13": math.radians(-90), "joint_23": math.radians(-90), 
+                "joint_33": math.radians(-90), "joint_43": math.radians(-90), 
+                "joint_53": math.radians(-90), "joint_63": math.radians(-90),
+            },
+            joint_vel={
+                # <<<--- 新增：显式设置关节初始速度为0
+                "joint_11": 0.0, "joint_21": 0.0, "joint_31": 0.0,
+                "joint_41": 0.0, "joint_51": 0.0, "joint_61": 0.0,
+                "joint_12": 0.0, "joint_22": 0.0, "joint_32": 0.0,
+                "joint_42": 0.0, "joint_52": 0.0, "joint_62": 0.0,
+                "joint_13": 0.0, "joint_23": 0.0, "joint_33": 0.0,
+                "joint_43": 0.0, "joint_53": 0.0, "joint_63": 0.0,
+            }
+        ),
+    actuators={
+        "all_joints": ImplicitActuatorCfg(
+            joint_names_expr=".*", 
+            stiffness=20.0,  # <<<--- 修复：增加刚度，提高稳定性
+            damping=12.0,     # <<<--- 修复：增加阻尼，减少振荡
+            effort_limit_sim=5.0,    # <<<--- 修复：增加扭矩限制
+            velocity_limit_sim=5.0  # <<<--- 修复：增加速度限制
+        )
+    }
+)
+    # -------- 传感器配置 --------
+    contact_sensor: ContactSensorCfg = ContactSensorCfg(
+        prim_path="/World/envs/env_.*/Robot/.*", history_length=3,update_period=0.005,
+        track_air_time=True
     )
 
-    # ─────────────────────────── 批量环境 ──────────────────────────
+    # --- 关节和连杆名称表达式 ---
+    toe_joint_names_expr: str | list[int] | None = "joint_.[3]"
+    undesired_contact_link_names_expr: str = "(thigh_link_[1-6]1|shin_link_[1-6]2)"
+    base_link_name: str = "base_link" # 用于检测基座触地
+
+    # -------- 场景配置 --------
     scene: InteractiveSceneCfg = InteractiveSceneCfg(
-        num_envs=2,                   # 2 个环境
+        num_envs=4096,
         env_spacing=4.0,
         replicate_physics=True,
     )
 
-    # ─────────────────────────── 其他超参 ──────────────────────────
-    action_scale: float = 0.5              # [-1,1] → ±0.5 rad
+    # -------- 离散指令配置 (Discrete Command Profile) --------
+    command_profile: dict = {
+        # 当机器人被指令移动时，奖励函数会参考这个速度，但不再强制精确匹配
+        # 可以理解为“期望的平均移动速度”或“最大期望速度”
+        "reference_linear_speed": 0.3,  # m/s
+        "reference_angular_rate": 0.4,  # rad/s
+        # 指令模式: (前后, 左右, 转向)。值为 -1, 0, 或 1。
+        # [1,0,0] 前进; [0,-1,0] 左移; [0,0,1] 右转
+        "command_mode_duration_s": 3.0, # 训练时一个指令模式持续的秒数
+        "stand_still_prob": 0.15, # 训练时指令为“站立不动”的概率
+        "num_command_modes": 7 # (站立, 前, 后, 左, 右, 左转, 右转)
+    }
+
+    # -------- 奖励缩放因子 --------
+    action_scale: float = 0.5
+
+    # --- 主要的正向激励 ---
+    rew_scale_move_in_commanded_direction: float = +2.5 # 奖励在指令方向上的移动
+    rew_scale_achieve_reference_angular_rate: float = +0.5 # 奖励达到参考角速度 (如果指令转向)
+    # 如果不追求特定线速度，可以移除或大幅减小此项，或者将其变为一个“不要太快”的惩罚
+    # rew_scale_achieve_reference_linear_speed: float = +0.5
+
+    rew_scale_alive: float = +0.05
+    rew_scale_target_height: float = +5.0
+    target_height_m: float = 0.2
+
+    # --- 行为平滑与效率相关的惩罚 ---
+    rew_scale_action_cost: float = -0.0002
+    rew_scale_action_rate: float = -0.02
+    rew_scale_joint_torques: float = -2.0e-5
+    rew_scale_joint_accel: float = -2.0e-7
+
+    # --- 姿态与运动稳定性相关的惩罚 ---
+    rew_scale_lin_vel_z_penalty: float = -2.0   # 惩罚Z轴线速度 (不希望上下跳动)
+    rew_scale_ang_vel_xy_penalty: float = -0.05 # 惩罚XY轴角速度 (不希望翻滚/侧倾)
+    rew_scale_flat_orientation: float = -5.0    # 惩罚身体不水平 (基于projected_gravity_b)
+    rew_scale_unwanted_movement_penalty: float = -2.0 # 当指令为静止时，惩罚移动
+
+    # --- 行为约束相关的惩罚 ---
+    rew_scale_dof_at_limit: float = -0.2
+    rew_scale_toe_orientation_penalty: float = -1.5
+    rew_scale_low_height_penalty: float = -25.0
+    min_height_penalty_threshold: float = 0.15
+
+    # --- 不期望的接触惩罚 ---
+    rew_scale_undesired_contact: float = -2.0   # 惩罚大腿/小腿触地
+
+    # --- 终止状态相关的惩罚 ---
+    rew_scale_termination: float = -15.0
 
-    rew_scale_upright:    float = +5.0
-    rew_scale_angvel:     float = -0.1
-    rew_scale_torque:     float = -2e-4
-    # rew_scale_collision:  float = -10.0
+    # -------- 重置随机化 --------
+    root_orientation_yaw_range: float = math.pi
+    reset_height_offset: float = 0.2
 
-    # 初始 root 姿态在 ±π 随机
-    root_orientation_range: float = math.pi
+    # -------- 终止条件 --------
+    termination_body_z_thresh: float = 0.98 # 基于projected_gravity_z
+    termination_height_thresh: float = 0.05
+    termination_base_contact: bool = True
\ No newline at end of file
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py
index 0bf3825..d922b99 100644
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py
+++ b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py
@@ -1,4 +1,4 @@
-# sixfeet_env_cfg.py (Simplified Commands, No root_lin_vel_b in obs)
+# sixfeet_env_cfg.py (Focus on Standing Up)
 from __future__ import annotations
 import math
 
@@ -16,10 +16,10 @@ from isaaclab.sensors import ContactSensorCfg
 class SixfeetEnvCfg(DirectRLEnvCfg):
     # -------- 基本环境配置 --------
     decimation: int = 2
-    episode_length_s: float = 20.0
+    episode_length_s: float = 20.0 # 可以适当缩短，如果只学站立
     action_space: int = 18
 
-    # ---- 新的观测空间定义 (移除 root_lin_vel_b) ----
+    # ---- 观测空间定义 ----
     # projected_gravity_b (3) + root_ang_vel_b (3) + discrete_commands (3)
     # + joint_pos_rel (18) + joint_vel (18)
     # Total = 3 + 3 + 3 + 18 + 18 = 45 dimensions
@@ -60,70 +60,37 @@ class SixfeetEnvCfg(DirectRLEnvCfg):
     )
 
     # -------- 机器人配置 --------
-    # robot: ArticulationCfg = ArticulationCfg(
-    #     prim_path="/World/envs/env_.*/Robot",
-    #     spawn=sim_utils.UsdFileCfg(
-    #         usd_path="/home/lee/EE_ws/src/sixfeet_src/sixfeet/source/sixfeet/sixfeet/assets/hexapod/hexapod.usd", # !! 确保路径正确 !!
-    #         activate_contact_sensors=True,
-    #         rigid_props=sim_utils.RigidBodyPropertiesCfg(
-    #             disable_gravity=None, max_depenetration_velocity=10.0, enable_gyroscopic_forces=True
-    #         ),
-    #         articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-    #             enabled_self_collisions=True, solver_position_iteration_count=12,
-    #             solver_velocity_iteration_count=1, sleep_threshold=0.005, stabilization_threshold=0.001,
-    #         ),
-    #     ),
-    #     init_state=ArticulationCfg.InitialStateCfg(
-    #         pos=(0.0, 0.0, 0.3),
-    #         # rot=(1.0, 0.0, 0.0, 0.0),
-    #         joint_pos={
-    #             "joint_11": 0.0, "joint_21": 0.0, "joint_31": 0.0, "joint_41": 0.0, "joint_51": 0.0, "joint_61": 0.0,
-    #             "joint_12": math.radians(45), "joint_22": math.radians(45), "joint_32": math.radians(45),
-    #             "joint_42": math.radians(45), "joint_52": math.radians(45), "joint_62": math.radians(45),
-    #             "joint_13": math.radians(-90), "joint_23": math.radians(-90), "joint_33": math.radians(-90),
-    #             "joint_43": math.radians(-90), "joint_53": math.radians(-90), "joint_63": math.radians(-90),
-    #         }
-    #     ),
-    #     actuators={
-    #         "all_joints": ImplicitActuatorCfg(
-    #             joint_names_expr=".*", stiffness=60.0,damping=4.0,
-    #             effort_limit_sim=6.0, velocity_limit_sim=8.0
-    #         )
-    #     }
-    # )
     robot: ArticulationCfg = ArticulationCfg(
         prim_path="/World/envs/env_.*/Robot",
         spawn=sim_utils.UsdFileCfg(
-            usd_path="/home/lee/EE_ws/src/sixfeet_src/sixfeet/source/sixfeet/sixfeet/assets/hexapod/hexapod.usd",
-            # usd_path="/home/lee/EE_ws/src/robot_urdf/urdf/hexapod_2/hexapod_2.usd",
+            usd_path="/home/lee/EE_ws/src/sixfeet_src/sixfeet/source/sixfeet/sixfeet/assets/hexapod/hexapod.usd", # 请确保路径正确!
             activate_contact_sensors=True,
             rigid_props=sim_utils.RigidBodyPropertiesCfg(
                 disable_gravity=False,
                 max_depenetration_velocity=10.0,
-                enable_gyroscopic_forces=False,
+                enable_gyroscopic_forces=False, # 通常设为False，除非特殊需求
             ),
             articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-                enabled_self_collisions=True,  # <<<--- 修复：可能导致不稳定
-                solver_position_iteration_count=12,
-                solver_velocity_iteration_count=1,
+                enabled_self_collisions=True,
+                solver_position_iteration_count=12, # 可以适当增加迭代次数提高稳定性
+                solver_velocity_iteration_count=1,  # 可以适当增加迭代次数提高稳定性
                 sleep_threshold=0.005,
                 stabilization_threshold=0.001,
             ),
         ),
         init_state=ArticulationCfg.InitialStateCfg(
-            pos=(0.0, 0.0, 0.15),
-            rot=(1.0, 0.0, 0.0, 0.0),  # <<<--- 修复：明确设置四元数
-            joint_pos={
+            pos=(0.0, 0.0, 0.15), # 初始高度较低，便于学习站起
+            rot=(1.0, 0.0, 0.0, 0.0), # 标准初始姿态 (w,x,y,z)
+            joint_pos={ # 初始关节角度，可以是一个趴下的姿态
                 "joint_11": 0.0, "joint_21": 0.0, "joint_31": 0.0, "joint_41": 0.0, "joint_51": 0.0, "joint_61": 0.0,
-                "joint_12": math.radians(45), "joint_22": math.radians(45), 
-                "joint_32": math.radians(45), "joint_42": math.radians(45), 
+                "joint_12": math.radians(45), "joint_22": math.radians(45),
+                "joint_32": math.radians(45), "joint_42": math.radians(45),
                 "joint_52": math.radians(45), "joint_62": math.radians(45),
-                "joint_13": math.radians(-90), "joint_23": math.radians(-90), 
-                "joint_33": math.radians(-90), "joint_43": math.radians(-90), 
+                "joint_13": math.radians(-90), "joint_23": math.radians(-90),
+                "joint_33": math.radians(-90), "joint_43": math.radians(-90),
                 "joint_53": math.radians(-90), "joint_63": math.radians(-90),
             },
-            joint_vel={
-                # <<<--- 新增：显式设置关节初始速度为0
+            joint_vel={ # 初始关节速度为0
                 "joint_11": 0.0, "joint_21": 0.0, "joint_31": 0.0,
                 "joint_41": 0.0, "joint_51": 0.0, "joint_61": 0.0,
                 "joint_12": 0.0, "joint_22": 0.0, "joint_32": 0.0,
@@ -132,16 +99,17 @@ class SixfeetEnvCfg(DirectRLEnvCfg):
                 "joint_43": 0.0, "joint_53": 0.0, "joint_63": 0.0,
             }
         ),
-    actuators={
-        "all_joints": ImplicitActuatorCfg(
-            joint_names_expr=".*", 
-            stiffness=20.0,  # <<<--- 修复：增加刚度，提高稳定性
-            damping=12.0,     # <<<--- 修复：增加阻尼，减少振荡
-            effort_limit_sim=5.0,    # <<<--- 修复：增加扭矩限制
-            velocity_limit_sim=5.0  # <<<--- 修复：增加速度限制
-        )
-    }
-)
+        actuators={
+            "all_joints": ImplicitActuatorCfg(
+                joint_names_expr=".*",
+                stiffness=20.0, # 驱动器P项
+                damping=12.0,   # 驱动器D项
+                effort_limit_sim=5.0, # 关节力矩限制
+                velocity_limit_sim=5.0 # 关节速度限制
+            )
+        }
+    )
+
     # -------- 传感器配置 --------
     contact_sensor: ContactSensorCfg = ContactSensorCfg(
         prim_path="/World/envs/env_.*/Robot/.*", history_length=3,update_period=0.005,
@@ -149,8 +117,8 @@ class SixfeetEnvCfg(DirectRLEnvCfg):
     )
 
     # --- 关节和连杆名称表达式 ---
-    toe_joint_names_expr: str | list[int] | None = "joint_.[3]"
-    undesired_contact_link_names_expr: str = "(thigh_link_[1-6]1|shin_link_[1-6]2)"
+    toe_joint_names_expr: str | list[int] | None = "joint_.[3]" # 用于足端方向惩罚
+    undesired_contact_link_names_expr: str = "(thigh_link_[1-6]1|shin_link_[1-6]2)" # 非足端接触惩罚
     base_link_name: str = "base_link" # 用于检测基座触地
 
     # -------- 场景配置 --------
@@ -161,60 +129,55 @@ class SixfeetEnvCfg(DirectRLEnvCfg):
     )
 
     # -------- 离散指令配置 (Discrete Command Profile) --------
+    # 为了只学习站立，我们将让机器人始终接收“站立”指令
     command_profile: dict = {
-        # 当机器人被指令移动时，奖励函数会参考这个速度，但不再强制精确匹配
-        # 可以理解为“期望的平均移动速度”或“最大期望速度”
-        "reference_linear_speed": 0.3,  # m/s
-        "reference_angular_rate": 0.4,  # rad/s
-        # 指令模式: (前后, 左右, 转向)。值为 -1, 0, 或 1。
-        # [1,0,0] 前进; [0,-1,0] 左移; [0,0,1] 右转
-        "command_mode_duration_s": 3.0, # 训练时一个指令模式持续的秒数
-        "stand_still_prob": 0.15, # 训练时指令为“站立不动”的概率
-        "num_command_modes": 7 # (站立, 前, 后, 左, 右, 左转, 右转)
+        "reference_linear_speed": 0.0,  # m/s (站立时参考速度为0)
+        "reference_angular_rate": 0.0,  # rad/s (站立时参考角速度为0)
+        "command_mode_duration_s": episode_length_s, # 指令持续整个episode
+        "stand_still_prob": 1.0,       # !!! 始终发出站立指令 !!!
+        "num_command_modes": 7 # (虽然概率为1，但结构保留)
     }
 
-    # -------- 奖励缩放因子 --------
+    # -------- 奖励缩放因子 (专注站立) --------
     action_scale: float = 0.5
 
-    # --- 主要的正向激励 ---
-    rew_scale_move_in_commanded_direction: float = +2.5 # 奖励在指令方向上的移动
-    rew_scale_achieve_reference_angular_rate: float = +0.5 # 奖励达到参考角速度 (如果指令转向)
-    # 如果不追求特定线速度，可以移除或大幅减小此项，或者将其变为一个“不要太快”的惩罚
-    # rew_scale_achieve_reference_linear_speed: float = +0.5
-
-    rew_scale_alive: float = +0.05
-    rew_scale_target_height: float = +5.0
-    target_height_m: float = 0.2
-
-    # --- 行为平滑与效率相关的惩罚 ---
-    rew_scale_action_cost: float = -0.0002
-    rew_scale_action_rate: float = -0.02
-    rew_scale_joint_torques: float = -2.0e-5
-    rew_scale_joint_accel: float = -2.0e-7
-
-    # --- 姿态与运动稳定性相关的惩罚 ---
-    rew_scale_lin_vel_z_penalty: float = -2.0   # 惩罚Z轴线速度 (不希望上下跳动)
-    rew_scale_ang_vel_xy_penalty: float = -0.05 # 惩罚XY轴角速度 (不希望翻滚/侧倾)
-    rew_scale_flat_orientation: float = -5.0    # 惩罚身体不水平 (基于projected_gravity_b)
-    rew_scale_unwanted_movement_penalty: float = -2.0 # 当指令为静止时，惩罚移动
-
-    # --- 行为约束相关的惩罚 ---
-    rew_scale_dof_at_limit: float = -0.2
-    rew_scale_toe_orientation_penalty: float = -1.5
-    rew_scale_low_height_penalty: float = -25.0
-    min_height_penalty_threshold: float = 0.15
-
-    # --- 不期望的接触惩罚 ---
-    rew_scale_undesired_contact: float = -2.0   # 惩罚大腿/小腿触地
+    # --- 主要的正向激励 (站立) ---
+    rew_scale_move_in_commanded_direction: float = 0.0  # !!! 禁用移动奖励 !!!
+    rew_scale_achieve_reference_angular_rate: float = 0.0 # !!! 禁用转向奖励 !!!
+
+    rew_scale_alive: float = +0.1 # 略微增加存活奖励，鼓励持续站立
+    rew_scale_target_height: float = +8.0 # !!! 增加目标高度的奖励权重 !!!
+    target_height_m: float = 0.20 # 目标站立高度 (米)
+
+    # --- 行为平滑与效率相关的惩罚 (可设为0或保留较小值) ---
+    rew_scale_action_cost: float = -0.0001 # 可以保留一个非常小的动作成本
+    rew_scale_action_rate: float = -0.01   # 可以保留一个小的动作变化率惩罚，使动作更平滑
+    rew_scale_joint_torques: float = -1.0e-6 # 极小的力矩惩罚
+    rew_scale_joint_accel: float = -1.0e-7   # 极小的加速度惩罚
+
+    # --- 姿态与运动稳定性相关的惩罚 (对站立很重要) ---
+    rew_scale_lin_vel_z_penalty: float = -3.0   # !!! 略微增加Z轴速度惩罚，避免上下晃动 !!!
+    rew_scale_ang_vel_xy_penalty: float = -0.1 # !!! 略微增加XY轴角速度惩罚，避免摇晃和摔倒 !!!
+    rew_scale_flat_orientation: float = -10.0    # !!! 增加身体水平的惩罚权重 !!!
+    rew_scale_unwanted_movement_penalty: float = -5.0 # !!! 增加在站立指令下移动的惩罚权重 !!!
+
+    # --- 行为约束相关的惩罚 (对站立很重要) ---
+    rew_scale_dof_at_limit: float = -0.5 # 略微增加关节到达极限的惩罚
+    rew_scale_toe_orientation_penalty: float = -2.0 # 增加足端不良姿态的惩罚
+    rew_scale_low_height_penalty: float = -30.0 # !!! 大幅增加低高度惩罚 !!!
+    min_height_penalty_threshold: float = 0.15 # 低于此高度则开始惩罚 (米)
+
+    # --- 不期望的接触惩罚 (对站立很重要) ---
+    rew_scale_undesired_contact: float = -5.0   # !!! 增加大腿/小腿触地的惩罚权重 !!!
 
     # --- 终止状态相关的惩罚 ---
-    rew_scale_termination: float = -15.0
+    rew_scale_termination: float = -20.0 # 失败终止的惩罚 (如摔倒)
 
     # -------- 重置随机化 --------
-    root_orientation_yaw_range: float = math.pi
-    reset_height_offset: float = 0.2
+    root_orientation_yaw_range: float = math.pi # 重置时初始Yaw角随机范围
+    reset_height_offset: float = 0.0 # 从较低的初始高度开始，不需要额外偏移
 
     # -------- 终止条件 --------
-    termination_body_z_thresh: float = 0.98 # 基于projected_gravity_z
-    termination_height_thresh: float = 0.05
-    termination_base_contact: bool = True
\ No newline at end of file
+    termination_body_z_thresh: float = 0.95 # 身体倾斜到一定程度终止 (值越小越容易因为倾斜终止)
+    termination_height_thresh: float = 0.05 # 高度过低终止
+    termination_base_contact: bool = True   # 身体躯干接触地面时终止
\ No newline at end of file