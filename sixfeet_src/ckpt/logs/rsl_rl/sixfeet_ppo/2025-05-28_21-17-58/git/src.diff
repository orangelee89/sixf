--- git status ---
On branch test1
Your branch and 'origin/test1' have diverged,
and have 4 and 1 different commits each, respectively.
  (use "git pull" to merge the remote branch into yours)

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   sixfeet_src/ckpt/.term_pid
	modified:   sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/events.out.tfevents.1748472972.TASL-1.323325.0
	modified:   sixfeet_src/ckpt/supervisor.sh
	modified:   sixfeet_src/ckpt/train_all.sh
	modified:   sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env.py
	modified:   sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/exported/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_3600.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_4000.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_4400.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_4800.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_5200.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_5600.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_6000.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/model_6400.pt
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_16-35-21/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_18-09-52/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_20-23-39/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_20-34-29/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_20-45-46/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_20-54-57/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_21-10-51/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_21-13-24/
	sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_21-17-58/
	sixfeet_src/ckpt/outputs/2025-05-28/16-35-21/
	sixfeet_src/ckpt/outputs/2025-05-28/17-15-14/
	sixfeet_src/ckpt/outputs/2025-05-28/17-17-24/
	sixfeet_src/ckpt/outputs/2025-05-28/17-27-56/
	sixfeet_src/ckpt/outputs/2025-05-28/17-28-33/
	sixfeet_src/ckpt/outputs/2025-05-28/17-32-43/
	sixfeet_src/ckpt/outputs/2025-05-28/17-34-17/
	sixfeet_src/ckpt/outputs/2025-05-28/18-09-52/
	sixfeet_src/ckpt/outputs/2025-05-28/18-50-31/
	sixfeet_src/ckpt/outputs/2025-05-28/18-56-57/
	sixfeet_src/ckpt/outputs/2025-05-28/20-07-44/
	sixfeet_src/ckpt/outputs/2025-05-28/20-09-58/
	sixfeet_src/ckpt/outputs/2025-05-28/20-20-02/
	sixfeet_src/ckpt/outputs/2025-05-28/20-23-39/
	sixfeet_src/ckpt/outputs/2025-05-28/20-34-29/
	sixfeet_src/ckpt/outputs/2025-05-28/20-45-46/
	sixfeet_src/ckpt/outputs/2025-05-28/20-54-57/
	sixfeet_src/ckpt/outputs/2025-05-28/21-10-51/
	sixfeet_src/ckpt/outputs/2025-05-28/21-13-24/
	sixfeet_src/ckpt/outputs/2025-05-28/21-17-58/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/sixfeet_src/ckpt/.term_pid b/sixfeet_src/ckpt/.term_pid
index 954fb92..12881e8 100644
--- a/sixfeet_src/ckpt/.term_pid
+++ b/sixfeet_src/ckpt/.term_pid
@@ -1 +1 @@
-323281
+421240
diff --git a/sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/events.out.tfevents.1748472972.TASL-1.323325.0 b/sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/events.out.tfevents.1748472972.TASL-1.323325.0
index 162afd9..fbce5e2 100644
Binary files a/sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/events.out.tfevents.1748472972.TASL-1.323325.0 and b/sixfeet_src/ckpt/logs/rsl_rl/sixfeet_ppo/2025-05-28_15-56-08/events.out.tfevents.1748472972.TASL-1.323325.0 differ
diff --git a/sixfeet_src/ckpt/supervisor.sh b/sixfeet_src/ckpt/supervisor.sh
index c2e015a..5c5c382 100755
--- a/sixfeet_src/ckpt/supervisor.sh
+++ b/sixfeet_src/ckpt/supervisor.sh
@@ -1,29 +1,25 @@
 #!/usr/bin/env bash
-# ========= train_supervisor.sh (V3 - Modified for Framework Choice) =========
+# ========= train_supervisor.sh (V3.1 - 支持用户输入训练单元数) =========
 set -euo pipefail
 
 ########## 可调参数 ##########
-GPU_UTIL_TH=40                # GPU 利用率阈值 (%)
-GPU_IDLE_SEC_THRESHOLD=10     # 连续空闲秒数 → 重启
-CHECK_INTERVAL=5              # 主循环检查间隔
-WARMUP_AFTER_LAUNCH=30        # 新启动后忽略 GPU 空闲的秒数
-# MAX_CONSECUTIVE_FAST_FAILURES 和 FAST_FAILURE_WINDOW_SEC 保留原样
+GPU_UTIL_TH=40
+GPU_IDLE_SEC_THRESHOLD=10
+CHECK_INTERVAL=5
+WARMUP_AFTER_LAUNCH=30
 MAX_CONSECUTIVE_FAST_FAILURES=3
 FAST_FAILURE_WINDOW_SEC=300
 ##############################################
 
 WS="/home/lee/EE_ws/src/sixfeet_src/ckpt"
 TRAIN_SH="$WS/train_all.sh"
-# 注意: PY_SCRIPT_NAME 和 PY_SCRIPT_PATTERN 可能需要根据实际Python脚本名调整
-# 如果两个框架的Python训练脚本都叫 train.py (在不同目录下)，这个可能仍然有效。
-# 否则，你可能需要一个更通用的模式，或者让 train_all.sh 将实际的脚本名/PID 传递出来。
 PY_SCRIPT_NAME="train.py"
-TERMINAL_TITLE="IsaacLab_Sixfeet_RL_Train" # 终端窗口标题
+TERMINAL_TITLE="IsaacLab_Sixfeet_RL_Train"
+PID_FILE="$WS/.term_pid"
+PY_SCRIPT_PATTERN="$PY_SCRIPT_NAME"
 
-PID_FILE="$WS/.term_pid"              # 存储终端进程PID的文件
-PY_SCRIPT_PATTERN="$PY_SCRIPT_NAME"   # 用于 pgrep 匹配 Python 训练进程
-
-FRAMEWORK_ARG="" # 用于存储用户选择的框架参数 (rl_games 或 rsl_rl)
+FRAMEWORK_ARG=""
+USER_DECLARED_TOTAL_TRAINING_UNITS="0" # 初始化为 "0"，表示使用 train_all.sh 的默认值
 
 ########## 选择 RL 框架 ##########
 select_framework() {
@@ -36,29 +32,36 @@ select_framework() {
     echo "------------------------------------------"
     read -r -p "请输入选项 (1 或 2): " choice
     case "$choice" in
-      1)
-        FRAMEWORK_ARG="rl_games"
-        echo "[$(date)] 已选择 RL-Games (传递给 train_all.sh 的参数: $FRAMEWORK_ARG)。"
-        break
-        ;;
-      2)
-        FRAMEWORK_ARG="rsl_rl"
-        echo "[$(date)] 已选择 RSL-RL (传递给 train_all.sh 的参数: $FRAMEWORK_ARG)。"
-        break
-        ;;
-      *)
-        echo "[$(date)] 无效选项 '$choice'。请重新输入。"
-        ;;
+      1) FRAMEWORK_ARG="rl_games"; echo "[$(date)] 已选择 RL-Games。"; break ;;
+      2) FRAMEWORK_ARG="rsl_rl"; echo "[$(date)] 已选择 RSL-RL。"; break ;;
+      *) echo "[$(date)] 无效选项 '$choice'。请重新输入。" ;;
     esac
   done
 }
 
+########## 提示用户输入训练单元数 ##########
+prompt_for_training_units() {
+  local units_input
+  echo "------------------------------------------"
+  read -r -p "请输入目标总训练单元数 (例如 10000, 直接回车或输入0则使用 train_all.sh 中的默认值): " units_input
+  if [[ "$units_input" =~ ^[1-9][0-9]*$ ]]; then # 检查是否为正整数
+    USER_DECLARED_TOTAL_TRAINING_UNITS="$units_input"
+    echo "[$(date)] 用户设定目标总训练单元数: $USER_DECLARED_TOTAL_TRAINING_UNITS"
+  elif [[ -z "$units_input" ]] || [[ "$units_input" == "0" ]]; then
+    USER_DECLARED_TOTAL_TRAINING_UNITS="0" # 明确传递 "0" 表示使用默认值
+    echo "[$(date)] 用户选择使用 train_all.sh 中的默认目标总训练单元数。"
+  else
+    USER_DECLARED_TOTAL_TRAINING_UNITS="0" # 无效输入也视为使用默认值
+    echo "[$(date)] 无效输入 '$units_input'。将使用 train_all.sh 中的默认目标总训练单元数。"
+  fi
+  echo "------------------------------------------"
+}
+
 ########## 清理函数 ##########
 cleanup_and_exit() {
   echo "[$(date)] Supervisor 退出，清理标志文件和终端..."
-  # 调用 kill_previous_session 来确保终端也被关闭
-  kill_previous_session || true # 忽略可能的错误，确保 rm 执行
-  rm -f "$WS/train_finished.flag" # PID_FILE 会在 kill_previous_session 中删除
+  kill_previous_session || true
+  rm -f "$WS/train_finished.flag"
   echo "[$(date)] 清理完成。"
   exit 0
 }
@@ -66,151 +69,109 @@ trap cleanup_and_exit SIGINT SIGTERM
 
 ########## 打开新终端 ##########
 open_new_terminal() {
-  echo "[$(date)] 打开新终端 ($TERMINAL_TITLE) 并使用框架 '$FRAMEWORK_ARG' 启动训练..."
-  # 将 FRAMEWORK_ARG 作为参数传递给 TRAIN_SH
+  echo "[$(date)] 打开新终端 ($TERMINAL_TITLE) 并使用框架 '$FRAMEWORK_ARG' 和训练单元数 '$USER_DECLARED_TOTAL_TRAINING_UNITS' 启动训练..."
+  # 将 FRAMEWORK_ARG 和 USER_DECLARED_TOTAL_TRAINING_UNITS 作为参数传递给 TRAIN_SH
   gnome-terminal --title="$TERMINAL_TITLE" --geometry=100x24 --working-directory="$WS" \
-    -- bash -c "$TRAIN_SH \"$FRAMEWORK_ARG\"; echo -e \"\n[$(date)] '$TRAIN_SH' 执行完毕。此终端将保持打开。\n按 Ctrl+D 或输入 exit 关闭此终端。\"; exec bash" &
+    -- bash -c "$TRAIN_SH \"$FRAMEWORK_ARG\" \"$USER_DECLARED_TOTAL_TRAINING_UNITS\"; echo -e \"\n[$(date)] '$TRAIN_SH' 执行完毕。此终端将保持打开。\n按 Ctrl+D 或输入 exit 关闭此终端。\"; exec bash" &
 
-  echo $! >"$PID_FILE"          # 记录 gnome-terminal 进程的 PID
-  sleep 3                      # 等待终端和命令启动
-  LAUNCH_TIME=$(date +%s)      # 更新启动时间
-  idle_seconds_counter=0       # 重置空闲计数器
+  echo $! >"$PID_FILE"
+  sleep 3
+  LAUNCH_TIME=$(date +%s)
+  idle_seconds_counter=0
 }
 
 ########## 杀掉旧会话 ##########
 kill_previous_session() {
   echo "[$(date)] 终止旧训练进程与终端..."
-
-  # 1) 杀掉 python 训练脚本和 train_all.sh 脚本
-  # 使用 pkill -f 来匹配完整命令行，避免误杀
   pkill -9 -f "$PY_SCRIPT_PATTERN" 2>/dev/null || echo "[$(date)] 未找到或无法终止 $PY_SCRIPT_PATTERN 进程。"
   pkill -9 -f "$TRAIN_SH"          2>/dev/null || echo "[$(date)] 未找到或无法终止 $TRAIN_SH 进程。"
-
-  # 2) 先尝试优雅关闭终端窗口 (如果窗口管理器工具可用)
   local terminal_closed_gracefully=false
   if command -v wmctrl &>/dev/null; then
     for win_id in $(wmctrl -l | grep "$TERMINAL_TITLE" | awk '{print $1}'); do
-      echo "[$(date)] 尝试通过 wmctrl 关闭窗口 ID: $win_id ..."
-      wmctrl -i -c "$win_id" && terminal_closed_gracefully=true
-      sleep 0.2 # 给窗口一点反应时间
+      echo "[$(date)] 尝试通过 wmctrl 关闭窗口 ID: $win_id ..."; wmctrl -i -c "$win_id" && terminal_closed_gracefully=true; sleep 0.2;
     done
   elif command -v xdotool &>/dev/null; then
     for win_id in $(xdotool search --onlyvisible --name "$TERMINAL_TITLE"); do
-      echo "[$(date)] 尝试通过 xdotool 关闭窗口 ID: $win_id ..."
-      xdotool windowclose "$win_id" && terminal_closed_gracefully=true
-      sleep 0.2 # 给窗口一点反应时间
+      echo "[$(date)] 尝试通过 xdotool 关闭窗口 ID: $win_id ..."; xdotool windowclose "$win_id" && terminal_closed_gracefully=true; sleep 0.2;
     done
   fi
-
-  if $terminal_closed_gracefully; then
-    echo "[$(date)] 已尝试优雅关闭匹配标题的终端窗口。"
-    sleep 0.8 # 等待窗口关闭过程
-  fi
-
-  # 3) 如果 PID 文件存在且进程存活，则强制 kill gnome-terminal 进程
+  if $terminal_closed_gracefully; then echo "[$(date)] 已尝试优雅关闭匹配标题的终端窗口。"; sleep 0.8; fi
   if [[ -f "$PID_FILE" ]]; then
-    TERM_PID_FROM_FILE=$(cat "$PID_FILE" || true) # 读取PID
-    if [[ -n "${TERM_PID_FROM_FILE:-}" ]] && ps -p "$TERM_PID_FROM_FILE" -o comm= | grep -q "gnome-terminal-" ; then # 检查是否是gnome-terminal进程且存活
-      echo "[$(date)] 终端窗口 '$TERMINAL_TITLE' (PID: $TERM_PID_FROM_FILE 来自文件) 似乎仍存活，发送 SIGTERM → SIGKILL ..."
-      kill -TERM "$TERM_PID_FROM_FILE" 2>/dev/null || true
-      sleep 0.8 # 等待 SIGTERM
-      kill -KILL "$TERM_PID_FROM_FILE" 2>/dev/null || true
-      echo "[$(date)] 已发送 SIGKILL 至 PID: $TERM_PID_FROM_FILE。"
-    else
-      echo "[$(date)] PID 文件中的 PID ($TERM_PID_FROM_FILE) 无效或进程已不存在。"
-    fi
-    rm -f "$PID_FILE" # 清理PID文件
-  else
-    echo "[$(date)] PID 文件 '$PID_FILE' 未找到。"
-  fi
-
-  # 4) 兜底：再次按标题模式查杀可能残留的 gnome-terminal 进程
-  # 这一步要非常小心，确保不会误杀其他窗口。
-  # pkill -f "gnome-terminal.*--title=$TERMINAL_TITLE" 是一种更精确的方式，但仍需谨慎。
-  # 为了更安全，可以考虑注释掉，或者只在特定情况下启用。
-  # 鉴于上面已经用了更精确的 PID 文件方法，这里的 pkill -9 -f "$TERMINAL_TITLE" 风险较高，已在原脚本中，此处保留。
-  # 如果需要更安全，应替换为更精确的 pgrep + kill 组合，或完全依赖PID文件。
+    TERM_PID_FROM_FILE=$(cat "$PID_FILE" || true)
+    if [[ -n "${TERM_PID_FROM_FILE:-}" ]] && ps -p "$TERM_PID_FROM_FILE" -o comm= | grep -q "gnome-terminal-" ; then
+      echo "[$(date)] 终端 PID: $TERM_PID_FROM_FILE 存活，发送 SIGTERM → SIGKILL ..."; kill -TERM "$TERM_PID_FROM_FILE" 2>/dev/null || true; sleep 0.8; kill -KILL "$TERM_PID_FROM_FILE" 2>/dev/null || true; echo "[$(date)] 已发送 SIGKILL 至 PID: $TERM_PID_FROM_FILE。";
+    else echo "[$(date)] PID 文件中的 PID ($TERM_PID_FROM_FILE) 无效或进程已不存在。"; fi
+    rm -f "$PID_FILE";
+  else echo "[$(date)] PID 文件 '$PID_FILE' 未找到。"; fi
   pkill -9 -f "$TERMINAL_TITLE" 2>/dev/null || echo "[$(date)] 尝试按标题 '$TERMINAL_TITLE' 清理终端进程，可能无匹配或已关闭。"
-
-
   echo "[$(date)] 旧会话清理完成。"
 }
 
 ########## 主逻辑 ##########
-# 1. 在脚本开始时让用户选择框架
+# 1. 用户选择框架
 select_framework
+# 2. 用户输入训练单元数
+prompt_for_training_units # <<< 新增调用
 
-# 2. 清理任何可能存在的旧会话
+# 3. 清理旧会话
 kill_previous_session
-sleep 2 # 清理后稍作等待，确保端口等资源释放
+sleep 2
 
-# 3. 打开新终端并开始第一次训练
+# 4. 打开新终端开始训练 (现在会传递训练单元数)
 open_new_terminal
 
-# 4. 主监控循环 (基本逻辑保持不变，但open_new_terminal会使用选定的框架)
+# 5. 主监控循环 (逻辑保持不变)
 while true; do
-  # A) 检查正常结束标志
   if [[ -f "$WS/train_finished.flag" ]]; then
     echo "[$(date)] train_finished.flag 检测到，训练完成。Supervisor 退出。"
-    cleanup_and_exit # 使用包含 kill_previous_session 的清理函数
+    cleanup_and_exit
   fi
-
   current_time_seconds=$(date +%s)
   time_since_launch=$((current_time_seconds - LAUNCH_TIME))
-
-  # B) 检查 Python 训练进程是否存活
-  # 同时检查 PY_SCRIPT_PATTERN (如 train.py) 和 TRAIN_SH (train_all.sh)
-  # 因为 train_all.sh 可能会在 python 脚本崩溃后很快退出
   python_process_alive=$(pgrep -f "$PY_SCRIPT_PATTERN" >/dev/null && echo 1 || echo 0)
   train_cmd_alive=$(pgrep -f "$TRAIN_SH" >/dev/null && echo 1 || echo 0)
 
-  if (( python_process_alive == 0 && train_cmd_alive == 0 )); then # 两者都不在了
+  if (( python_process_alive == 0 && train_cmd_alive == 0 )); then
     if (( time_since_launch > WARMUP_AFTER_LAUNCH )); then
-      echo "[$(date)] 训练进程 ($PY_SCRIPT_NAME 和 $TRAIN_SH) 未找到 (已运行 ${time_since_launch}s)，判定为崩溃，准备重启..."
-      idle_seconds_counter=$GPU_IDLE_SEC_THRESHOLD # 直接触发重启条件
+      echo "[$(date)] 训练进程未找到 (已运行 ${time_since_launch}s)，判定为崩溃，准备重启..."
+      idle_seconds_counter=$GPU_IDLE_SEC_THRESHOLD
     elif (( time_since_launch <= WARMUP_AFTER_LAUNCH )); then
-       echo "[$(date)] 预热期内训练进程 ($PY_SCRIPT_NAME 和 $TRAIN_SH) 退出，立即重启..."
-       kill_previous_session
-       sleep 3
-       # 预热期崩溃，默认使用上次的选择，如果需要重新选择，取消下一行注释
-       # select_framework
-       open_new_terminal
-       # WARMUP_AFTER_LAUNCH 和 LAUNCH_TIME 会在 open_new_terminal 中重置
-       sleep "$CHECK_INTERVAL" # 等待下一次检查
-       continue
+       echo "[$(date)] 预热期内训练进程退出，立即重启..."
+       kill_previous_session; sleep 3;
+       # select_framework # 如果需要在每次预热期崩溃后都重新选择，则取消注释
+       # prompt_for_training_units # 如果需要每次预热期崩溃后都重新输入，则取消注释
+       open_new_terminal;
+       sleep "$CHECK_INTERVAL"; continue;
     fi
-  else # 至少有一个脚本在运行，检查 GPU 利用率 (仅在预热期后)
+  else
     if (( time_since_launch > WARMUP_AFTER_LAUNCH )); then
       gpu_util=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits 2>/dev/null | head -n1 || echo 0)
       if (( gpu_util <= GPU_UTIL_TH )); then
-        if (( python_process_alive == 1 )); then # Python 脚本还在，但GPU空闲
+        if (( python_process_alive == 1 )); then
             idle_seconds_counter=$((idle_seconds_counter + CHECK_INTERVAL))
             echo "[$(date)] GPU util ${gpu_util}% ≤ ${GPU_UTIL_TH}% (Python 进程存活, idle ${idle_seconds_counter}/${GPU_IDLE_SEC_THRESHOLD}s)"
-        else # Python 脚本不在了，但 train_all.sh 可能还在 (不太可能，但作为一种情况)
-            echo "[$(date)] Python 进程 ($PY_SCRIPT_PATTERN) 未找到，但 train_all.sh 存活。GPU util ${gpu_util}%。计入空闲。"
+        else
+            echo "[$(date)] Python 进程未找到，但 train_all.sh 存活。GPU util ${gpu_util}%。计入空闲。"
             idle_seconds_counter=$((idle_seconds_counter + CHECK_INTERVAL))
         fi
       else
-        # GPU 忙碌 (且至少一个相关脚本存活)，重置空_idle_seconds_counter=0
+        idle_seconds_counter=0
         echo "[$(date)] GPU util ${gpu_util}% > ${GPU_UTIL_TH}%. 训练正常。"
       fi
     else
-      # 在预热期内，只要进程活着就不增加 idle_seconds_counter
       idle_seconds_counter=0
       echo "[$(date)] 预热期 (${time_since_launch}s / ${WARMUP_AFTER_LAUNCH}s), 脚本进程存活。"
     fi
   fi
 
-  # D) 达到空闲/崩溃重启阈值
   if (( idle_seconds_counter >= GPU_IDLE_SEC_THRESHOLD )); then
     if (( time_since_launch > WARMUP_AFTER_LAUNCH )); then
         echo "[$(date)] GPU 持续空闲或进程异常 (已运行 ${time_since_launch}s)，重启训练..."
-        kill_previous_session
-        sleep 3
-        # select_framework # 同上，决定是否在每次重启时都重新选择框架
+        kill_previous_session; sleep 3;
+        # select_framework # 同上
+        # prompt_for_training_units # 同上
         open_new_terminal
     fi
   fi
-
   sleep "$CHECK_INTERVAL"
 done
\ No newline at end of file
diff --git a/sixfeet_src/ckpt/train_all.sh b/sixfeet_src/ckpt/train_all.sh
index 1c4fdca..e54fcb2 100755
--- a/sixfeet_src/ckpt/train_all.sh
+++ b/sixfeet_src/ckpt/train_all.sh
@@ -1,29 +1,51 @@
 #!/usr/bin/env bash
-# ========= train_all.sh (V3 – 支持 RL-Games / RSL-RL) =========
+# ========= train_all.sh (V3.2 – 接收并处理 supervisor 传递的训练单元数) =========
 set -euo pipefail
 
 ########################################
 # 0) 基本环境准备
 ########################################
-if [[ $# -lt 1 ]]; then
-  echo "用法: $0 <rsl_rl | rl_games>"
+if [[ $# -lt 1 ]]; then # 至少需要框架参数
+  echo "用法: $0 <rsl_rl | rl_games> [total_training_units]"
+  echo "  [total_training_units] 是可选的。如果提供并为正整数，则用作目标总迭代数。"
+  echo "  如果为0、空或无效，则使用脚本内定义的默认值。"
   exit 1
 fi
 SELECTED_FRAMEWORK="$1"
+# 从第二个参数获取用户提供的训练单元数，如果未提供，默认为 "0"
+USER_PROVIDED_TRAINING_UNITS_ARG="${2:-0}"
 
 source ~/miniconda3/etc/profile.d/conda.sh
-conda activate env_isaaclab
+conda activate env_isaaclab # 确保这是你正确的conda环境名
 WS1="/home/lee/EE_ws/src/sixfeet_src/ckpt"
 WS="/home/lee/EE_ws/src/sixfeet_src/sixfeet"
 cd "$WS1"
 
-TOTAL_TRAINING_UNITS=10000         # 视作总训练步数 / 迭代数
+# 脚本内定义的默认总训练单元数
+DEFAULT_INTERNAL_TOTAL_TRAINING_UNITS=10000
+TOTAL_TRAINING_UNITS=$DEFAULT_INTERNAL_TOTAL_TRAINING_UNITS # 初始化为内部默认值
+
+# 检查并使用从 supervisor 传递过来的 TOTAL_TRAINING_UNITS 参数
+if [[ "$USER_PROVIDED_TRAINING_UNITS_ARG" =~ ^[1-9][0-9]*$ ]]; then # 如果是有效的正整数
+    TOTAL_TRAINING_UNITS="$USER_PROVIDED_TRAINING_UNITS_ARG"
+    echo "[$(date)] INFO: 使用 supervisor 传递的目标总训练单元数: $TOTAL_TRAINING_UNITS"
+elif [[ "$USER_PROVIDED_TRAINING_UNITS_ARG" == "0" ]]; then # "0" 表示用户选择使用脚本内默认值
+    echo "[$(date)] INFO: Supervisor 请求使用 train_all.sh 中的默认目标总训练单元数: $TOTAL_TRAINING_UNITS (为 $DEFAULT_INTERNAL_TOTAL_TRAINING_UNITS)"
+else # 其他无效输入
+    echo "[$(date)] WARNING: 从 supervisor 收到无效的目标总训练单元数参数 ('$USER_PROVIDED_TRAINING_UNITS_ARG')。将使用默认值: $TOTAL_TRAINING_UNITS (为 $DEFAULT_INTERNAL_TOTAL_TRAINING_UNITS)"
+fi
+
 TASK_NAME="Template-Sixfeet-Direct-v0" # Isaac Lab task
 
 ########################################
 # 1) 选择框架并构造命令
 ########################################
 PYTHON_CMD_EXEC=()
+# 本次脚本执行的迭代次数
+# 对于新训练，它等于上面确定的 TOTAL_TRAINING_UNITS
+# 对于继续训练，它将是 TOTAL_TRAINING_UNITS 减去已完成的迭代数
+iterations_for_this_run=$TOTAL_TRAINING_UNITS
+
 
 if [[ "$SELECTED_FRAMEWORK" == "rsl_rl" ]]; then
   # ---------- RSL-RL ----------
@@ -33,12 +55,12 @@ if [[ "$SELECTED_FRAMEWORK" == "rsl_rl" ]]; then
 
   mkdir -p "$LOG_DIR"
   ckpt_path=$(ls -1t "$LOG_DIR"/*/model_*.pt 2>/dev/null | head -n1 || true)
+
   if [[ -n "$ckpt_path" ]]; then
     run_name=$(basename "$(dirname "$ckpt_path")")
     ckpt_file=$(basename "$ckpt_path")
-    echo "[$(date)] 检测到 RSL-RL checkpoint: $ckpt_path — 继续训练"
+    echo "[$(date)] 检测到 RSL-RL checkpoint: $ckpt_path"
 
-# 从 ckpt_file (例如 model_499.pt) 中提取数字部分 499
     completed_iterations_str=""
     if [[ "$ckpt_file" =~ model_([0-9]+)\.pt ]]; then
       completed_iterations_str="${BASH_REMATCH[1]}"
@@ -51,51 +73,53 @@ if [[ "$SELECTED_FRAMEWORK" == "rsl_rl" ]]; then
         iterations_for_this_run=$((TOTAL_TRAINING_UNITS - completed_iterations))
         echo "[$(date)] 目标总迭代数: $TOTAL_TRAINING_UNITS. 本次继续训练将运行剩余的 $iterations_for_this_run 迭代."
       else
-        echo "[$(date)] 目标总迭代数 $TOTAL_TRAINING_UNITS 已达到或超过。本次运行迭代数设为默认值."
+        iterations_for_this_run=0 # 已达到或超过目标，本次不运行或运行0次迭代
+        echo "[$(date)] 目标总迭代数 $TOTAL_TRAINING_UNITS 已达到或超过checkpoint中的 $completed_iterations 迭代。本次运行迭代数设为0."
       fi
     else
       echo "[$(date)] 警告: 无法从checkpoint文件名 '$ckpt_file' 中解析出有效的已完成迭代数。"
-      echo "[$(date)] 将默认运行 DECLARED_TOTAL_TRAINING_UNITS ($DECLARED_TOTAL_TRAINING_UNITS) 指定的迭代次数（如果train.py支持）。"
-      # iterations_for_this_run 保持为 DECLARED_TOTAL_TRAINING_UNITS
-      # 或者你可以选择在这里设置一个较小的默认值，或直接退出
+      echo "[$(date)] 将默认基于已确定的 iterations_for_this_run ($iterations_for_this_run) 执行（这通常意味着按总目标减去0，即整个目标长度）。"
+      # iterations_for_this_run 此时等于 TOTAL_TRAINING_UNITS
     fi
     
-    echo "[$(date)] — 继续训练，运行 $iterations_for_this_run 迭代。"
+    echo "[$(date)] — 继续训练 (RSL-RL)，计划运行 $iterations_for_this_run 迭代。"
     PYTHON_CMD_EXEC=(python -u "$PY_SCRIPT"            \
         --task "$TASK_NAME"                            \
         --resume                                      \
         --experiment_name "$EXP_NAME"                 \
-        --max_iterations "$iterations_for_this_run" \
         --load_run "$run_name"                        \
         --checkpoint "$ckpt_file"                     \
+        --max_iterations "$iterations_for_this_run"   \
         --headless)
   else
-    echo "[$(date)] 未找到 RSL-RL checkpoint — 从头开始训练"
+    echo "[$(date)] 未找到 RSL-RL checkpoint — 从头开始训练，计划运行 $iterations_for_this_run 迭代。"
     PYTHON_CMD_EXEC=(python -u "$PY_SCRIPT"            \
         --task "$TASK_NAME"                            \
         --experiment_name "$EXP_NAME"                 \
-        --max_iterations "$TOTAL_TRAINING_UNITS" \
+        --max_iterations "$iterations_for_this_run" \
         --headless)
   fi
 
 elif [[ "$SELECTED_FRAMEWORK" == "rl_games" ]]; then
   # ---------- RL-Games ----------
+  # RL-Games 的迭代控制通常由其配置文件中的 train_steps 或 max_epochs 决定。
+  # --checkpoint 参数使其从检查点加载，然后它会继续跑到配置中定义的总训练量。
+  # 如果要传递类似 "本次运行多少步" 的参数，需要你的 rl_games/train.py 支持这个。
+  # 目前，我们假设它会自行处理。
   LOG_ROOT="$WS1/logs/rl_games"
-  EXP_NAME="sixfeet_ppo"
+  EXP_NAME="sixfeet_ppo" 
   PY_SCRIPT="$WS/scripts/rl_games/train.py"
 
-  # 1) 先找 best —— 文件名固定为 sixfeet_ppo.pth
-best_ckpt=$(ls -1t "$LOG_ROOT/$EXP_NAME"/*/nn/sixfeet_ppo.pth 2>/dev/null | head -n1 || true)
+  best_ckpt=$(ls -1t "$LOG_ROOT/$EXP_NAME"/*/nn/sixfeet_ppo.pth 2>/dev/null | head -n1 || true)
 
-if [[ -n "$best_ckpt" ]]; then
-  ckpt="$best_ckpt"
-  echo "[$(date)] 选用 BEST checkpoint: $ckpt"
-else
-  # 2) 若无 best，再找最新 last_*.pth
-  ckpt=$(ls -1t "$LOG_ROOT/$EXP_NAME"/*/nn/last_*.pth 2>/dev/null | head -n1 || true)
-  [[ -n "$ckpt" ]] \
-    && echo "[$(date)] 未找到 sixfeet_ppo.pth，改用最新 last_: $ckpt"
-fi
+  if [[ -n "$best_ckpt" ]]; then
+    ckpt="$best_ckpt"
+    echo "[$(date)] 选用 RL-Games BEST checkpoint: $ckpt"
+  else
+    ckpt=$(ls -1t "$LOG_ROOT/$EXP_NAME"/*/nn/last_*.pth 2>/dev/null | head -n1 || true)
+    [[ -n "$ckpt" ]] \
+      && echo "[$(date)] 未找到 RL-Games sixfeet_ppo.pth，改用最新 last_: $ckpt"
+  fi
 
   if [[ -n "$ckpt" ]]; then
     echo "[$(date)] 检测到 RL-Games checkpoint: $ckpt — 继续训练"
@@ -132,4 +156,4 @@ else
   echo "[$(date)] Python 训练脚本失败，退出码: $PYTHON_EXIT_CODE"
 fi
 
-exit $PYTHON_EXIT_CODE
+exit $PYTHON_EXIT_CODE
\ No newline at end of file
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env.py
index 1139aa5..7c798e0 100644
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env.py
+++ b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env.py
@@ -12,12 +12,9 @@ from isaaclab.sensors import ContactSensor
 from isaaclab.utils.math import (
     quat_rotate,
     quat_from_angle_axis,
-    # quat_mul, # 如果下面的RPY转换不理想，可能需要这个
-    # quat_from_euler_xyz, # Isaac Lab 可能没有直接提供这个，但有类似工具
     convert_quat,
     euler_xyz_from_quat
 )
-# from isaaclab.terrains import TerrainImporter
 
 from .sixfeet_env_cfg import SixfeetEnvCfg
 
@@ -28,145 +25,105 @@ def normalize_angle_for_obs(x: torch.Tensor) -> torch.Tensor:
 
 @torch.jit.script
 def compute_sixfeet_rewards_directional(
-    # ... (大部分现有参数不变) ...
-    root_lin_vel_b: torch.Tensor,
-    root_ang_vel_b: torch.Tensor,
-    projected_gravity_b: torch.Tensor, # 世界Z轴在机器人本体坐标系下的投影 (取反后归一化)
-    joint_pos_rel: torch.Tensor,
-    joint_vel: torch.Tensor,
-    applied_torque: torch.Tensor,
-    joint_acc: torch.Tensor,
-    q_lower_limits: torch.Tensor,
-    q_upper_limits: torch.Tensor,
-    current_joint_pos_abs: torch.Tensor,
-    actions_from_policy: torch.Tensor,
-    previous_actions_from_policy: torch.Tensor,
-    root_pos_w: torch.Tensor,
-    undesired_contacts_active: torch.Tensor,
-    commands_discrete: torch.Tensor,
-    cfg_cmd_profile: Dict[str, float],
-    cfg_rew_scale_move_in_commanded_direction: float,
-    cfg_rew_scale_achieve_reference_angular_rate: float,
-    cfg_rew_scale_alive: float,
-    cfg_rew_scale_target_height: float,
-    cfg_target_height_m: float,
-    cfg_rew_scale_action_cost: float,
-    cfg_rew_scale_action_rate: float,
-    cfg_rew_scale_joint_torques: float,
-    cfg_rew_scale_joint_accel: float,
-    cfg_rew_scale_lin_vel_z_penalty: float,
-    cfg_rew_scale_ang_vel_xy_penalty: float,
-    cfg_rew_scale_flat_orientation: float,
-    cfg_rew_scale_unwanted_movement_penalty: float,
-    cfg_rew_scale_dof_at_limit: float,
-    cfg_rew_scale_toe_orientation_penalty: float,
-    cfg_toe_joint_indices: Optional[torch.Tensor],
-    cfg_rew_scale_low_height_penalty: float,
-    cfg_min_height_penalty_threshold: float,
-    cfg_rew_scale_undesired_contact: float,
-    dt: float,
+    # ... (参数列表与上一版本相同，直到 cfg_rew_scale_self_collision) ...
+    root_lin_vel_b: torch.Tensor, root_ang_vel_b: torch.Tensor, projected_gravity_b: torch.Tensor,
+    joint_pos_rel: torch.Tensor, joint_vel: torch.Tensor, applied_torque: torch.Tensor,
+    joint_acc: torch.Tensor, q_lower_limits: torch.Tensor, q_upper_limits: torch.Tensor,
+    current_joint_pos_abs: torch.Tensor, actions_from_policy: torch.Tensor,
+    previous_actions_from_policy: torch.Tensor, root_pos_w: torch.Tensor,
+    undesired_contacts_active: torch.Tensor, commands_discrete: torch.Tensor,
+    cfg_cmd_profile: Dict[str, float], cfg_rew_scale_move_in_commanded_direction: float,
+    cfg_rew_scale_achieve_reference_angular_rate: float, cfg_rew_scale_alive: float,
+    cfg_rew_scale_target_height: float, cfg_target_height_m: float,
+    cfg_rew_scale_action_cost: float, cfg_rew_scale_action_rate: float,
+    cfg_rew_scale_joint_torques: float, cfg_rew_scale_joint_accel: float,
+    cfg_rew_scale_lin_vel_z_penalty: float, cfg_rew_scale_ang_vel_xy_penalty: float,
+    cfg_rew_scale_flat_orientation: float, cfg_rew_scale_unwanted_movement_penalty: float,
+    cfg_rew_scale_dof_at_limit: float, cfg_rew_scale_toe_orientation_penalty: float,
+    cfg_toe_joint_indices: Optional[torch.Tensor], cfg_rew_scale_low_height_penalty: float,
+    cfg_min_height_penalty_threshold: float, cfg_rew_scale_undesired_contact: float,
+    dt: float, cfg_rew_scale_orientation_deviation: float,
+    cfg_orientation_termination_angle_limit_rad: float,
+    cfg_joint_limit_penalty_threshold_percent: float,
+    num_self_collisions_per_env: torch.Tensor, # 来自上一轮的自碰撞
+    cfg_rew_scale_self_collision: float,       # 来自上一轮的自碰撞
     # --- 新增参数 ---
-    cfg_rew_scale_orientation_deviation: float # 新的Z轴偏差惩罚的scale
+    was_severely_tilted_last_step: torch.Tensor, # (num_envs,) bool, 上一步是否严重倾斜
+    cfg_rew_scale_successful_flip: float       # 成功翻转的奖励scale
 ) -> tuple[torch.Tensor, Dict[str, torch.Tensor]]:
 
+    # ... (所有现有的奖励计算逻辑，如移动、存活、条件高度、动作、效率、稳定性惩罚等，保持不变) ...
+    # (为了简洁，这里省略这些部分的重复代码，它们与你文件中的一致)
+    # 例如:
     ref_ang_rate = cfg_cmd_profile.get("reference_angular_rate", 0.0)
-
-    # 1. Movement Rewards
-    linear_vel_x_local = root_lin_vel_b[:, 0]
-    linear_vel_y_local = root_lin_vel_b[:, 1]
-    reward_fwd_bkwd_move = commands_discrete[:, 0] * linear_vel_x_local
-    reward_left_right_move = commands_discrete[:, 1] * linear_vel_y_local
+    linear_vel_x_local = root_lin_vel_b[:, 0]; linear_vel_y_local = root_lin_vel_b[:, 1]
+    reward_fwd_bkwd_move = commands_discrete[:, 0] * linear_vel_x_local; reward_left_right_move = commands_discrete[:, 1] * linear_vel_y_local
     is_linear_cmd_active = (torch.abs(commands_discrete[:, 0]) > 0.5) | (torch.abs(commands_discrete[:, 1]) > 0.5)
     reward_linear_direction = (reward_fwd_bkwd_move + reward_left_right_move) * is_linear_cmd_active.float()
     reward_move_in_commanded_direction = reward_linear_direction * cfg_rew_scale_move_in_commanded_direction
-    angular_vel_z_local = root_ang_vel_b[:, 2]
-    reward_angular_direction_raw = -commands_discrete[:, 2] * angular_vel_z_local
+    angular_vel_z_local = root_ang_vel_b[:, 2]; reward_angular_direction_raw = -commands_discrete[:, 2] * angular_vel_z_local
     is_turn_cmd_active = torch.abs(commands_discrete[:, 2]) > 0.5
     turn_rate_error = torch.abs(torch.abs(angular_vel_z_local) - ref_ang_rate)
-    reward_achieve_ref_ang_rate = torch.exp(-5.0 * turn_rate_error) * is_turn_cmd_active.float() \
-                                   * cfg_rew_scale_achieve_reference_angular_rate
-    reward_turn = (reward_angular_direction_raw * is_turn_cmd_active.float() * cfg_rew_scale_move_in_commanded_direction) + \
-                  reward_achieve_ref_ang_rate
-
-    # 2. Alive and Height Rewards
+    reward_achieve_ref_ang_rate = torch.exp(-5.0 * turn_rate_error) * is_turn_cmd_active.float() * cfg_rew_scale_achieve_reference_angular_rate
+    reward_turn = (reward_angular_direction_raw * is_turn_cmd_active.float() * cfg_rew_scale_move_in_commanded_direction) + reward_achieve_ref_ang_rate
     reward_alive = torch.ones_like(commands_discrete[:,0]) * cfg_rew_scale_alive
     current_height_z = root_pos_w[:, 2]
+    cos_angle_robot_z_with_world_z_cond = -projected_gravity_b[:, 2]
+    device = projected_gravity_b.device
+    is_within_orientation_limit_for_rewards = cos_angle_robot_z_with_world_z_cond >= torch.cos(torch.tensor(cfg_orientation_termination_angle_limit_rad, device=device))
     height_check = torch.clamp(current_height_z / cfg_target_height_m, max=1.1)
-    reward_target_height = height_check * cfg_rew_scale_target_height
-
-    # 3. Action Penalties
+    _base_reward_target_height = height_check * cfg_rew_scale_target_height
+    reward_target_height = torch.where(is_within_orientation_limit_for_rewards, _base_reward_target_height, torch.zeros_like(_base_reward_target_height))
     penalty_action_cost = torch.sum(actions_from_policy**2, dim=-1) * cfg_rew_scale_action_cost
     penalty_action_rate = torch.sum((actions_from_policy - previous_actions_from_policy)**2, dim=-1) * cfg_rew_scale_action_rate
-
-    # 4. Efficiency Penalties
     penalty_joint_torques = torch.sum(applied_torque**2, dim=-1) * cfg_rew_scale_joint_torques
     penalty_joint_accel = torch.sum(joint_acc**2, dim=-1) * cfg_rew_scale_joint_accel
-    
-    # 5. Stability Penalties
     penalty_lin_vel_z = torch.square(root_lin_vel_b[:, 2]) * cfg_rew_scale_lin_vel_z_penalty
     penalty_ang_vel_xy = torch.sum(torch.square(root_ang_vel_b[:, :2]), dim=1) * cfg_rew_scale_ang_vel_xy_penalty
-    penalty_flat_orientation = torch.sum(torch.square(projected_gravity_b[:, :2]), dim=1) * cfg_rew_scale_flat_orientation # 惩罚XY方向的重力投影，即身体不水平
+    penalty_flat_orientation = torch.sum(torch.square(projected_gravity_b[:, :2]), dim=1) * cfg_rew_scale_flat_orientation
     is_stand_cmd_active = torch.all(commands_discrete == 0, dim=1)
     unwanted_lin_vel_sq = torch.sum(torch.square(root_lin_vel_b[:, :2]), dim=1)
     unwanted_ang_vel_sq = torch.square(root_ang_vel_b[:, 2])
-    penalty_unwanted_movement = (unwanted_lin_vel_sq + unwanted_ang_vel_sq) * \
-                                is_stand_cmd_active.float() * cfg_rew_scale_unwanted_movement_penalty
-
-    # 6. Constraint Penalties
+    penalty_unwanted_movement = (unwanted_lin_vel_sq + unwanted_ang_vel_sq) * is_stand_cmd_active.float() * cfg_rew_scale_unwanted_movement_penalty
     dof_range = q_upper_limits - q_lower_limits
     dof_range = torch.where(dof_range < 1e-6, torch.ones_like(dof_range), dof_range)
     q_lower_expanded = q_lower_limits.unsqueeze(0) if q_lower_limits.ndim == 1 else q_lower_limits
     dof_range_expanded = dof_range.unsqueeze(0) if dof_range.ndim == 1 else dof_range
     dof_pos_scaled_01 = (current_joint_pos_abs - q_lower_expanded) / dof_range_expanded
-    near_lower_limit = torch.relu(0.05 - dof_pos_scaled_01)**2
-    near_upper_limit = torch.relu(dof_pos_scaled_01 - 0.95)**2
+    threshold_percent = cfg_joint_limit_penalty_threshold_percent
+    near_lower_limit = torch.relu(threshold_percent - dof_pos_scaled_01)**2
+    near_upper_limit = torch.relu(dof_pos_scaled_01 - (1.0 - threshold_percent))**2
     penalty_dof_at_limit = torch.sum(near_lower_limit + near_upper_limit, dim=-1) * cfg_rew_scale_dof_at_limit
-    
-    # --- 条件化惩罚逻辑 ---
-    # is_severely_tilted: 机器人 Z 轴是否大致朝下 (与世界Z轴夹角 > 90度)
-    is_severely_tilted = projected_gravity_b[:, 2] > 0.0
-
-    # 条件化足端方向惩罚
-    _base_penalty_toe_orientation = torch.zeros_like(commands_discrete[:,0], device=commands_discrete.device)
+    is_severely_tilted_rewards = projected_gravity_b[:, 2] > 0.0 # 用于奖励的条件判断
+    _base_penalty_toe_orientation = torch.zeros_like(commands_discrete[:,0], device=device)
     if cfg_rew_scale_toe_orientation_penalty != 0.0 and cfg_toe_joint_indices is not None:
         if cfg_toe_joint_indices.numel() > 0:
             toe_joint_positions = current_joint_pos_abs[:, cfg_toe_joint_indices]
             _base_penalty_toe_orientation = torch.sum(torch.relu(toe_joint_positions)**2, dim=-1) * cfg_rew_scale_toe_orientation_penalty
-    penalty_toe_orientation = torch.where(
-        is_severely_tilted, 
-        torch.zeros_like(_base_penalty_toe_orientation), 
-        _base_penalty_toe_orientation
-    )
-    
-    # 低高度惩罚 (这个不受 is_severely_tilted 影响，除非你也想改)
+    penalty_toe_orientation = torch.where(is_severely_tilted_rewards, torch.zeros_like(_base_penalty_toe_orientation), _base_penalty_toe_orientation)
     is_too_low = (current_height_z < cfg_min_height_penalty_threshold).float()
     penalty_low_height = is_too_low * cfg_rew_scale_low_height_penalty
-
-    # 条件化不期望的接触惩罚
     _base_penalty_undesired_contact = undesired_contacts_active.float() * cfg_rew_scale_undesired_contact
-    penalty_undesired_contact = torch.where(
-        is_severely_tilted,
-        torch.zeros_like(_base_penalty_undesired_contact),
-        _base_penalty_undesired_contact
-    )
-
-    # --- 新增：Z轴方向偏差惩罚 ---
-    # projected_gravity_b[:, 2] 的范围是 [-1, 1]
-    # -1: 机器人Z轴与世界Z轴同向 (直立)
-    # +1: 机器人Z轴与世界Z轴反向 (倒置)
-    # acos的输入范围是 [-1, 1]。为防止计算误差导致超出范围，进行clamp。
-    cos_angle_robot_z_with_world_z = -projected_gravity_b[:, 2] # 这是机器人Z轴与世界Z轴点积
-    angle_deviation_from_world_z = torch.acos(torch.clamp(cos_angle_robot_z_with_world_z, -1.0 + 1e-7, 1.0 - 1e-7)) # 弧度制, 范围 [0, pi]
-    # 当直立时，angle_deviation_from_world_z 接近 0
-    # 当倒置时，angle_deviation_from_world_z 接近 pi
-    # scale 应该是负的，所以偏差越大，惩罚越大（负的越多）
+    penalty_undesired_contact = torch.where(is_severely_tilted_rewards, torch.zeros_like(_base_penalty_undesired_contact), _base_penalty_undesired_contact)
+    cos_angle_robot_z_with_world_z_dev = -projected_gravity_b[:, 2]
+    angle_deviation_from_world_z = torch.acos(torch.clamp(cos_angle_robot_z_with_world_z_dev, -1.0 + 1e-7, 1.0 - 1e-7))
     penalty_orientation_deviation = cfg_rew_scale_orientation_deviation * angle_deviation_from_world_z
-    
+    penalty_self_collision = cfg_rew_scale_self_collision * (num_self_collisions_per_env > 0).float()
+
+
+    # --- 新增：成功翻转奖励 ---
+    current_is_severely_tilted = projected_gravity_b[:, 2] > 0.0 # 当前是否严重倾斜 (Z轴大致朝下)
+    # transitioned_to_upright_hemisphere: 上一步严重倾斜，这一步不再严重倾斜 (即Z轴进入了上半球)
+    transitioned_to_upright_hemisphere = was_severely_tilted_last_step & (~current_is_severely_tilted)
+    reward_successful_flip = cfg_rew_scale_successful_flip * transitioned_to_upright_hemisphere.float()
+
+
     # --- 总奖励计算 ---
-    # penalty_orientation_deviation 不乘以 dt，直接作用
-    # penalty_flat_orientation 保持在你基准文件中的处理方式（乘以dt）
+    # 将 reward_successful_flip 加入到不乘以 dt 的组
     total_reward = (
-        reward_move_in_commanded_direction + reward_turn + reward_alive + reward_target_height + penalty_orientation_deviation
+        reward_move_in_commanded_direction + reward_turn + reward_alive + reward_target_height 
+        + penalty_orientation_deviation + penalty_self_collision 
+        + reward_successful_flip # <--- 新增翻转奖励
         + (penalty_action_cost + penalty_action_rate + penalty_joint_torques + penalty_joint_accel
         + penalty_lin_vel_z + penalty_ang_vel_xy + penalty_flat_orientation + penalty_unwanted_movement
         + penalty_dof_at_limit + penalty_toe_orientation + penalty_low_height
@@ -174,21 +131,25 @@ def compute_sixfeet_rewards_directional(
     )
     
     reward_terms: Dict[str, torch.Tensor] = {
+        # ... (所有其他现有日志项) ...
+        "orientation_deviation_penalty": penalty_orientation_deviation,
+        "self_collision_penalty": penalty_self_collision,
+        "successful_flip_reward": reward_successful_flip, # <--- 新增日志项
+        # ... (其他乘以 dt 的项) ...
         "move_in_commanded_direction": reward_move_in_commanded_direction,
         "turn_reward_combined": reward_turn,
         "alive": reward_alive,
         "target_height": reward_target_height,
-        "orientation_deviation_penalty": penalty_orientation_deviation, # 新增
         "action_cost_penalty": penalty_action_cost * dt,
         "action_rate_penalty": penalty_action_rate * dt,
         "joint_torques_penalty": penalty_joint_torques * dt,
         "joint_accel_penalty": penalty_joint_accel * dt,
         "lin_vel_z_penalty": penalty_lin_vel_z * dt,
         "ang_vel_xy_penalty": penalty_ang_vel_xy * dt,
-        "flat_orientation_penalty": penalty_flat_orientation * dt, # 按你的基准文件处理
+        "flat_orientation_penalty": penalty_flat_orientation * dt,
         "unwanted_movement_penalty": penalty_unwanted_movement * dt,
         "dof_at_limit_penalty": penalty_dof_at_limit * dt,
-        "toe_orientation_penalty": penalty_toe_orientation * dt, 
+        "toe_orientation_penalty": penalty_toe_orientation * dt,
         "low_height_penalty": penalty_low_height * dt,
         "undesired_contact_penalty": penalty_undesired_contact * dt,
     }
@@ -198,356 +159,341 @@ def compute_sixfeet_rewards_directional(
 class SixfeetEnv(DirectRLEnv):
     cfg: SixfeetEnvCfg
     _contact_sensor: ContactSensor
+    _orientation_termination_angle_limit_rad: float
+    _was_severely_tilted_last_step: torch.Tensor # 新增状态变量
 
     def __init__(self, cfg: SixfeetEnvCfg, render_mode: str | None = None, **kwargs):
         super().__init__(cfg, render_mode, **kwargs)
+        
+        if hasattr(self.cfg, "orientation_termination_angle_limit_deg"):
+            self._orientation_termination_angle_limit_rad = math.radians(self.cfg.orientation_termination_angle_limit_deg)
+        else: 
+            self._orientation_termination_angle_limit_rad = math.radians(90.0) # 默认值
+
+        # 初始化 _was_severely_tilted_last_step
+        # 假设初始时机器人可能不是严重倾斜的，或者在第一次 _get_rewards 前会被重置。
+        # 为了避免在第一个step因为初始状态是好的而错误地触发翻转奖励，
+        # 可以在 _reset_idx 中根据初始姿态正确设置它。
+        # 简单起见，先初始化为 False。如果reset时是倾斜的，会在第一次get_rewards后被正确更新。
+        # 或者初始化为 True，以避免在reset到一个好姿态时立即获得翻转奖励。
+        self._was_severely_tilted_last_step = torch.ones(self.num_envs, device=self.device, dtype=torch.bool)
+
 
+        # ... (其他 __init__ 内容与上一版本相同，包括 _default_joint_pos, _q_limits 等) ...
         self._default_joint_pos = self.robot.data.default_joint_pos.clone()
         if self._default_joint_pos.ndim > 1 and self._default_joint_pos.shape[0] == self.num_envs:
             self._default_joint_pos = self._default_joint_pos[0]
-        
         joint_limits = self.robot.data.joint_pos_limits[0].to(self.device)
-        self._q_lower_limits = joint_limits[:, 0]
-        self._q_upper_limits = joint_limits[:, 1]
-        
+        self._q_lower_limits = joint_limits[:, 0]; self._q_upper_limits = joint_limits[:, 1]
         self._policy_actions = torch.zeros(self.num_envs, self.cfg.action_space, device=self.device)
         self._previous_policy_actions = torch.zeros_like(self._policy_actions)
         self._processed_actions = torch.zeros_like(self._policy_actions)
-
         self._commands = torch.zeros(self.num_envs, 3, device=self.device, dtype=torch.float)
         self._time_since_last_command_change = torch.zeros(self.num_envs, device=self.device)
-        
-        self._resolve_toe_joint_indices() # num_dof 修正已在方法内
-
+        self._resolve_toe_joint_indices()
         self._undesired_contact_body_ids: Optional[List[int]] = None
-        if self.cfg.undesired_contact_link_names_expr and self.cfg.rew_scale_undesired_contact != 0.0:
+        if hasattr(self.cfg, "undesired_contact_link_names_expr") and self.cfg.undesired_contact_link_names_expr and \
+           hasattr(self.cfg, "rew_scale_undesired_contact") and self.cfg.rew_scale_undesired_contact != 0.0:
             indices, names = self._contact_sensor.find_bodies(self.cfg.undesired_contact_link_names_expr)
-            if indices:
-                self._undesired_contact_body_ids = indices
-                print(f"[INFO] SixfeetEnv: Undesired contact body IDs: {self._undesired_contact_body_ids} for names {names}")
-            else:
-                print(f"[WARNING] SixfeetEnv: No bodies for undesired contact expr: {self.cfg.undesired_contact_link_names_expr}")
-
+            if indices: self._undesired_contact_body_ids = indices; print(f"[INFO] Undesired contact IDs: {indices} for {names}")
+            else: print(f"[WARNING] No bodies for undesired contact: {self.cfg.undesired_contact_link_names_expr}")
         self._base_body_id: Optional[List[int]] = None
-        if self.cfg.termination_base_contact and self.cfg.base_link_name:
+        if hasattr(self.cfg, "termination_base_contact") and self.cfg.termination_base_contact and \
+           hasattr(self.cfg, "base_link_name") and self.cfg.base_link_name:
             indices, names = self._contact_sensor.find_bodies(self.cfg.base_link_name)
-            if indices:
-                self._base_body_id = indices
-                print(f"[INFO] SixfeetEnv: Base body ID for termination: {self._base_body_id} for names {names}")
-            else:
-                print(f"[WARNING] SixfeetEnv: No body for base contact termination: {self.cfg.base_link_name}")
-        
+            if indices: self._base_body_id = indices; print(f"[INFO] Base body ID: {indices} for {names}")
+            else: print(f"[WARNING] No body for base contact: {self.cfg.base_link_name}")
         self._episode_reward_terms_sum: Dict[str, torch.Tensor] = {}
 
-    def _resolve_toe_joint_indices(self):
+
+    # ... (_resolve_toe_joint_indices, _setup_scene, _update_commands, _pre_physics_step, _apply_action, _get_observations 与上一版本相同) ...
+    # (确保这些方法使用的是你提供的 "Corrected JIT Type Hints" 版本中的逻辑)
+    def _resolve_toe_joint_indices(self): # 与你提供的版本一致 (num_dof 已修正)
         self._toe_joint_indices: Optional[torch.Tensor] = None
         expr_or_list = getattr(self.cfg, 'toe_joint_names_expr', None)
-        if self.cfg.rew_scale_toe_orientation_penalty == 0.0 or not expr_or_list:
-            return
-
-        num_dof_val = self._q_lower_limits.numel() # 使用 _q_lower_limits 获取 DoF 数量
+        if not hasattr(self.cfg, "rew_scale_toe_orientation_penalty") or \
+           self.cfg.rew_scale_toe_orientation_penalty == 0.0 or not expr_or_list: return
+        num_dof_val = self._q_lower_limits.numel() 
         joint_names_list_for_logging = []
-
         if isinstance(expr_or_list, str):
             joint_indices_list, joint_names_list_for_logging = self.robot.find_joints(expr_or_list)
-            if joint_indices_list:
-                self._toe_joint_indices = torch.tensor(joint_indices_list, device=self.device, dtype=torch.long)
+            if joint_indices_list: self._toe_joint_indices = torch.tensor(joint_indices_list, device=self.device, dtype=torch.long)
         elif isinstance(expr_or_list, list) and all(isinstance(i, int) for i in expr_or_list):
             if expr_or_list:
                 temp_indices = torch.tensor(expr_or_list, device=self.device, dtype=torch.long)
-                if torch.any(temp_indices < 0) or torch.any(temp_indices >= num_dof_val): # 使用 num_dof_val
-                    print(f"[ERROR] SixfeetEnv: Invalid toe joint indices in list: {expr_or_list}. Max allowable index: {num_dof_val - 1}")
-                    self._toe_joint_indices = None
-                else:
-                    self._toe_joint_indices = temp_indices
-        elif expr_or_list is not None:
-            print(f"[WARNING] SixfeetEnv: 'toe_joint_names_expr' ('{expr_or_list}') in cfg has invalid type: {type(expr_or_list)}. Expected str or list[int].")
-
+                if torch.any(temp_indices < 0) or torch.any(temp_indices >= num_dof_val):
+                    print(f"[ERROR] Invalid toe joint indices in list: {expr_or_list}. Max: {num_dof_val - 1}"); self._toe_joint_indices = None
+                else: self._toe_joint_indices = temp_indices
+        elif expr_or_list is not None: print(f"[WARNING] 'toe_joint_names_expr' ('{expr_or_list}') invalid type.")
         if self._toe_joint_indices is not None:
-            if self._toe_joint_indices.numel() == 0:
-                self._toe_joint_indices = None
-            elif torch.any(self._toe_joint_indices < 0) or torch.any(self._toe_joint_indices >= num_dof_val): # 使用 num_dof_val
-                print(f"[ERROR] SixfeetEnv: Invalid toe joint indices after processing: {self._toe_joint_indices.tolist()}. Max allowable index: {num_dof_val - 1}")
-                self._toe_joint_indices = None
-            else:
-                log_msg = f"[INFO] SixfeetEnv: Validated toe joint indices for penalty: {self._toe_joint_indices.tolist()}"
-                if joint_names_list_for_logging:
-                    log_msg += f", names: {joint_names_list_for_logging}"
+            if self._toe_joint_indices.numel() == 0: self._toe_joint_indices = None
+            elif torch.any(self._toe_joint_indices < 0) or torch.any(self._toe_joint_indices >= num_dof_val):
+                print(f"[ERROR] Invalid toe joint indices after processing: {self._toe_joint_indices.tolist()}. Max: {num_dof_val - 1}"); self._toe_joint_indices = None
+            else: 
+                log_msg = f"[INFO] Validated toe joint indices: {self._toe_joint_indices.tolist()}"
+                if joint_names_list_for_logging: log_msg += f", names: {joint_names_list_for_logging}"
                 print(log_msg)
-        
-        if self._toe_joint_indices is None and expr_or_list is not None:
-             print(f"[WARNING] SixfeetEnv: No valid toe joint indices resolved from '{expr_or_list}'. Toe orientation penalty might not apply effectively.")
-        elif self._toe_joint_indices is None and expr_or_list is None and self.cfg.rew_scale_toe_orientation_penalty != 0.0:
-             print(f"[INFO] SixfeetEnv: 'toe_joint_names_expr' not specified, but toe penalty > 0. Toe orientation penalty will not be applied.")
+        if self._toe_joint_indices is None and expr_or_list is not None: print(f"[WARNING] No valid toe joint indices from '{expr_or_list}'.")
+        elif self._toe_joint_indices is None and expr_or_list is None and self.cfg.rew_scale_toe_orientation_penalty != 0.0: print(f"[INFO] Toe penalty active but no expr.")
 
     def _setup_scene(self): # 与你提供的版本一致
-        self.robot = Articulation(self.cfg.robot)
-        self.scene.articulations["robot"] = self.robot
-        self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
-        self.scene.sensors["contact_sensor"] = self._contact_sensor
+        self.robot = Articulation(self.cfg.robot); self.scene.articulations["robot"] = self.robot
+        self._contact_sensor = ContactSensor(self.cfg.contact_sensor); self.scene.sensors["contact_sensor"] = self._contact_sensor
         if hasattr(self.cfg, "terrain") and self.cfg.terrain is not None:
-            if hasattr(self.scene, "cfg"):
-                self.cfg.terrain.num_envs = self.scene.cfg.num_envs
-                self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
+            if hasattr(self.scene, "cfg"): self.cfg.terrain.num_envs = self.scene.cfg.num_envs; self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
             terrain_class_path = getattr(self.cfg.terrain, "class_type", None)
             if isinstance(terrain_class_path, str):
-                try:
-                    module_path, class_name = terrain_class_path.rsplit('.', 1)
-                    module = __import__(module_path, fromlist=[class_name])
-                    terrain_class = getattr(module, class_name)
-                except Exception as e:
-                    print(f"[ERROR] Failed to import terrain class {terrain_class_path}: {e}")
-                    from isaaclab.terrains import TerrainImporter
-                    terrain_class = TerrainImporter
-            elif terrain_class_path is None:
-                from isaaclab.terrains import TerrainImporter
-                terrain_class = TerrainImporter
-            else:
-                terrain_class = terrain_class_path
+                try: module_path, class_name = terrain_class_path.rsplit('.', 1); module = __import__(module_path, fromlist=[class_name]); terrain_class = getattr(module, class_name)
+                except Exception as e: print(f"[ERROR] Failed to import terrain class {terrain_class_path}: {e}"); from isaaclab.terrains import TerrainImporter; terrain_class = TerrainImporter
+            elif terrain_class_path is None: from isaaclab.terrains import TerrainImporter; terrain_class = TerrainImporter
+            else: terrain_class = terrain_class_path
             self._terrain = terrain_class(self.cfg.terrain)
         else:
-            print("[WARNING] SixfeetEnv: No terrain configuration. Spawning default plane.")
-            from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
-            spawn_ground_plane("/World/ground", GroundPlaneCfg())
-            class DummyTerrain:
+            print("[WARNING] SixfeetEnv: No terrain cfg. Spawning default plane."); from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane; spawn_ground_plane("/World/ground", GroundPlaneCfg())
+            class DummyTerrain: 
                 def __init__(self, num_envs, device): self.env_origins = torch.zeros((num_envs, 3), device=device)
             self._terrain = DummyTerrain(self.cfg.scene.num_envs, self.device)
-        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
-        light_cfg.func("/World/Light", light_cfg)
+        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75,0.75,0.75)); light_cfg.func("/World/Light", light_cfg)
         self.scene.clone_environments(copy_from_source=False)
 
     def _update_commands(self, env_ids: torch.Tensor): # 与你提供的版本一致
         self._time_since_last_command_change[env_ids] += self.physics_dt
-        cmd_duration_str = str(self.cfg.command_profile.get("command_mode_duration_s", "20.0"))
-        if cmd_duration_str == "episode_length_s":
-            cmd_duration = self.cfg.episode_length_s
+        cmd_profile = self.cfg.command_profile
+        cmd_duration_s_config = cmd_profile.get("command_mode_duration_s", self.cfg.episode_length_s if hasattr(self.cfg, "episode_length_s") else 20.0)
+        if isinstance(cmd_duration_s_config, str) and cmd_duration_s_config == "episode_length_s": cmd_duration = self.cfg.episode_length_s
         else:
-            cmd_duration = float(cmd_duration_str)
-
-        stand_still_prob = self.cfg.command_profile.get("stand_still_prob", 0.0)
-        num_cmd_modes = self.cfg.command_profile.get("num_command_modes", 1)
+            try: cmd_duration = float(cmd_duration_s_config)
+            except (ValueError, TypeError) : cmd_duration = self.cfg.episode_length_s if hasattr(self.cfg, "episode_length_s") else 20.0
+        stand_still_prob = cmd_profile.get("stand_still_prob", 0.0); num_cmd_modes = cmd_profile.get("num_command_modes", 1)
         change_command_mask = self._time_since_last_command_change[env_ids] >= cmd_duration
         envs_to_change = env_ids[change_command_mask]
         if envs_to_change.numel() > 0:
-            self._time_since_last_command_change[envs_to_change] = 0.0
-            num_to_change = envs_to_change.shape[0]
+            self._time_since_last_command_change[envs_to_change] = 0.0; num_to_change = envs_to_change.shape[0]
             new_commands_for_changed_envs = torch.zeros(num_to_change, 3, device=self.device, dtype=torch.float)
-            if stand_still_prob == 1.0:
-                command_modes = torch.zeros(num_to_change, device=self.device, dtype=torch.long)
+            if stand_still_prob == 1.0: command_modes = torch.zeros(num_to_change, device=self.device, dtype=torch.long)
             elif num_cmd_modes > 0:
                 command_modes = torch.randint(0, num_cmd_modes, (num_to_change,), device=self.device)
-                if stand_still_prob > 0.0 and stand_still_prob < 1.0:
-                    stand_mask = torch.rand(num_to_change, device=self.device) < stand_still_prob
-                    command_modes[stand_mask] = 0
-            else:
-                command_modes = torch.zeros(num_to_change, device=self.device, dtype=torch.long)
-            new_commands_for_changed_envs[command_modes == 1, 0] = 1.0
-            new_commands_for_changed_envs[command_modes == 2, 0] = -1.0
-            new_commands_for_changed_envs[command_modes == 3, 1] = -1.0
-            new_commands_for_changed_envs[command_modes == 4, 1] = 1.0
-            new_commands_for_changed_envs[command_modes == 5, 2] = -1.0
-            new_commands_for_changed_envs[command_modes == 6, 2] = 1.0
+                if stand_still_prob > 0.0 and stand_still_prob < 1.0: stand_mask = torch.rand(num_to_change, device=self.device) < stand_still_prob; command_modes[stand_mask] = 0
+            else: command_modes = torch.zeros(num_to_change, device=self.device, dtype=torch.long)
+            new_commands_for_changed_envs[command_modes == 1, 0] = 1.0; new_commands_for_changed_envs[command_modes == 2, 0] = -1.0
+            new_commands_for_changed_envs[command_modes == 3, 1] = -1.0; new_commands_for_changed_envs[command_modes == 4, 1] = 1.0
+            new_commands_for_changed_envs[command_modes == 5, 2] = -1.0; new_commands_for_changed_envs[command_modes == 6, 2] = 1.0
             self._commands[envs_to_change] = new_commands_for_changed_envs
 
     def _pre_physics_step(self, actions: torch.Tensor): # 与你提供的版本一致
         self._policy_actions = actions.clone().to(self.device)
-        if torch.any(torch.isnan(actions)) or torch.any(torch.isinf(actions)):
-            print(f"[WARNING] Invalid actions detected: {actions}")
-            actions = torch.zeros_like(actions)
-        cur_pos = self.robot.data.joint_pos
-        self._processed_actions = cur_pos + self.cfg.action_scale * self._policy_actions
-        self._processed_actions = torch.clamp(
-            self._processed_actions,
-            self._q_lower_limits.unsqueeze(0),
-            self._q_upper_limits.unsqueeze(0)
-        )
-        all_env_ids = torch.arange(self.num_envs, device=self.device, dtype=torch.long)
-        self._update_commands(all_env_ids)
+        if torch.any(torch.isnan(actions)) or torch.any(torch.isinf(actions)): print(f"[WARNING] Invalid actions: {actions}"); actions = torch.zeros_like(actions)
+        cur_pos = self.robot.data.joint_pos; self._processed_actions = cur_pos + self.cfg.action_scale * self._policy_actions
+        self._processed_actions = torch.clamp(self._processed_actions, self._q_lower_limits.unsqueeze(0), self._q_upper_limits.unsqueeze(0))
+        all_env_ids = torch.arange(self.num_envs, device=self.device, dtype=torch.long); self._update_commands(all_env_ids)
 
-    def _apply_action(self): # 与你提供的版本一致
-        self.robot.set_joint_position_target(self._processed_actions)
+    def _apply_action(self): self.robot.set_joint_position_target(self._processed_actions)
 
     def _get_observations(self) -> dict: # 与你提供的版本一致
         self._previous_policy_actions = self._policy_actions.clone()
         default_pos_expanded = self._default_joint_pos.unsqueeze(0) if self._default_joint_pos.ndim == 1 else self._default_joint_pos
         joint_pos_rel = self.robot.data.joint_pos - default_pos_expanded
-        obs_list = [
-            self.robot.data.projected_gravity_b,
-            self.robot.data.root_ang_vel_b,
-            self._commands,
-            normalize_angle_for_obs(joint_pos_rel),
-            self.robot.data.joint_vel,
-        ]
+        obs_list = [self.robot.data.projected_gravity_b, self.robot.data.root_ang_vel_b, self._commands, normalize_angle_for_obs(joint_pos_rel), self.robot.data.joint_vel]
         observations_tensor = torch.cat(obs_list, dim=-1)
-        if hasattr(self.cfg, "observation_space") and observations_tensor.shape[1] != self.cfg.observation_space:
-            print(f"[ERROR] SixfeetEnv: Obs dim mismatch! Expected {self.cfg.observation_space}, got {observations_tensor.shape[1]}")
+        if hasattr(self.cfg, "observation_space") and observations_tensor.shape[1] != self.cfg.observation_space: print(f"[ERROR] Obs dim mismatch! Exp {self.cfg.observation_space}, got {observations_tensor.shape[1]}")
         return {"policy": observations_tensor}
 
     def _get_rewards(self) -> torch.Tensor:
-        root_lin_vel_b = self.robot.data.root_lin_vel_b
-        root_ang_vel_b = self.robot.data.root_ang_vel_b
+        # --- 获取状态 ---
+        root_lin_vel_b = self.robot.data.root_lin_vel_b; root_ang_vel_b = self.robot.data.root_ang_vel_b
         projected_gravity_b = self.robot.data.projected_gravity_b
         default_pos_expanded = self._default_joint_pos.unsqueeze(0) if self._default_joint_pos.ndim == 1 else self._default_joint_pos
         joint_pos_rel = self.robot.data.joint_pos - default_pos_expanded
-        current_joint_pos_abs = self.robot.data.joint_pos
-        joint_vel = self.robot.data.joint_vel
-        applied_torque = self.robot.data.applied_torque
-        joint_acc = getattr(self.robot.data, "joint_acc", torch.zeros_like(joint_vel, device=self.device))
+        current_joint_pos_abs = self.robot.data.joint_pos; joint_vel = self.robot.data.joint_vel
+        applied_torque = self.robot.data.applied_torque; joint_acc = getattr(self.robot.data, "joint_acc", torch.zeros_like(joint_vel, device=self.device))
         root_pos_w = self.robot.data.root_pos_w
 
+        # --- 不期望的接触 ---
         undesired_contacts_active = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)
-        if self.cfg.rew_scale_undesired_contact != 0.0 and self._undesired_contact_body_ids and len(self._undesired_contact_body_ids) > 0:
+        if hasattr(self.cfg, "rew_scale_undesired_contact") and self.cfg.rew_scale_undesired_contact != 0.0 and \
+           self._undesired_contact_body_ids and len(self._undesired_contact_body_ids) > 0:
              if hasattr(self._contact_sensor.data, 'net_forces_w_history') and self._contact_sensor.data.net_forces_w_history is not None:
-                all_forces_history = self._contact_sensor.data.net_forces_w_history
-                if all_forces_history.ndim == 4 and all_forces_history.shape[1] > 0 and \
-                   self._undesired_contact_body_ids and max(self._undesired_contact_body_ids) < all_forces_history.shape[2]: # 安全检查
-                    current_net_forces_w = all_forces_history[:, -1, :, :]
-                    forces_on_undesired_bodies = current_net_forces_w[:, self._undesired_contact_body_ids, :]
-                    force_magnitudes = torch.norm(forces_on_undesired_bodies, dim=-1)
-                    undesired_contacts_active = torch.any(force_magnitudes > 1.0, dim=1)
+                all_forces = self._contact_sensor.data.net_forces_w_history
+                if all_forces.ndim == 4 and all_forces.shape[1] > 0 and self._undesired_contact_body_ids and max(self._undesired_contact_body_ids) < all_forces.shape[2]:
+                    forces = all_forces[:, -1, self._undesired_contact_body_ids, :]; undesired_contacts_active = torch.any(torch.norm(forces, dim=-1) > 1.0, dim=1)
+
+        # --- 计算自碰撞次数 (使用 ContactSensor) ---
+        num_self_collisions_per_env = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)
+        contact_data = self._contact_sensor.data
+        if hasattr(contact_data, 'body_indices_in_contact_buffer') and contact_data.body_indices_in_contact_buffer is not None and \
+           hasattr(self.robot.data, 'body_indices') and self.robot.data.body_indices is not None:
+            contact_pairs_global_indices = contact_data.body_indices_in_contact_buffer
+            robot_global_body_indices = self.robot.data.body_indices
+            if robot_global_body_indices.numel() > 0 and contact_pairs_global_indices.numel() > 0:
+                num_envs_contact, max_contacts_per_env, _ = contact_pairs_global_indices.shape
+                num_envs_robot, num_robot_bodies = robot_global_body_indices.shape
+                if num_envs_contact == self.num_envs and num_envs_robot == self.num_envs:
+                    robot_indices_expanded = robot_global_body_indices.unsqueeze(1)
+                    body0_global_idx_pairs = contact_pairs_global_indices[..., 0].unsqueeze(-1)
+                    body1_global_idx_pairs = contact_pairs_global_indices[..., 1].unsqueeze(-1)
+                    is_body0_robot_contact = torch.any(body0_global_idx_pairs == robot_indices_expanded, dim=2)
+                    is_body1_robot_contact = torch.any(body1_global_idx_pairs == robot_indices_expanded, dim=2)
+                    is_self_collision_pair = is_body0_robot_contact & is_body1_robot_contact
+                    valid_contact_indices = (contact_pairs_global_indices[..., 0] != -1) & \
+                                            (contact_pairs_global_indices[..., 1] != -1) & \
+                                            (contact_pairs_global_indices[..., 0] != contact_pairs_global_indices[..., 1])
+                    if hasattr(contact_data, 'num_contacts_in_buffer') and contact_data.num_contacts_in_buffer is not None:
+                        valid_contact_mask_sensor = torch.arange(max_contacts_per_env, device=self.device).unsqueeze(0) < contact_data.num_contacts_in_buffer.unsqueeze(1)
+                        is_self_collision_pair_valid = is_self_collision_pair & valid_contact_mask_sensor & valid_contact_indices
+                    else: 
+                        is_self_collision_pair_valid = is_self_collision_pair & valid_contact_indices
+                    num_self_collisions_per_env = torch.sum(is_self_collision_pair_valid, dim=1)
         
+        # --- 计算当前是否严重倾斜，用于成功翻转奖励 ---
+        current_is_severely_tilted = (projected_gravity_b[:, 2] > 0.0)
+
         total_reward, reward_terms_dict = compute_sixfeet_rewards_directional(
-            root_lin_vel_b, root_ang_vel_b, projected_gravity_b,
-            joint_pos_rel, joint_vel, applied_torque, joint_acc,
-            self._q_lower_limits, self._q_upper_limits, current_joint_pos_abs,
-            self._policy_actions, self._previous_policy_actions,
-            root_pos_w, undesired_contacts_active, self._commands,
-            self.cfg.command_profile,
+            root_lin_vel_b, root_ang_vel_b, projected_gravity_b, joint_pos_rel, joint_vel, applied_torque, joint_acc,
+            self._q_lower_limits, self._q_upper_limits, current_joint_pos_abs, self._policy_actions, self._previous_policy_actions,
+            root_pos_w, undesired_contacts_active, self._commands, self.cfg.command_profile,
             self.cfg.rew_scale_move_in_commanded_direction, self.cfg.rew_scale_achieve_reference_angular_rate,
             self.cfg.rew_scale_alive, self.cfg.rew_scale_target_height, self.cfg.target_height_m,
-            self.cfg.rew_scale_action_cost, self.cfg.rew_scale_action_rate,
-            self.cfg.rew_scale_joint_torques, self.cfg.rew_scale_joint_accel,
-            self.cfg.rew_scale_lin_vel_z_penalty, self.cfg.rew_scale_ang_vel_xy_penalty,
-            self.cfg.rew_scale_flat_orientation, self.cfg.rew_scale_unwanted_movement_penalty,
-            self.cfg.rew_scale_dof_at_limit, self.cfg.rew_scale_toe_orientation_penalty, self._toe_joint_indices,
-            self.cfg.rew_scale_low_height_penalty, self.cfg.min_height_penalty_threshold,
+            self.cfg.rew_scale_action_cost, self.cfg.rew_scale_action_rate, self.cfg.rew_scale_joint_torques, self.cfg.rew_scale_joint_accel,
+            self.cfg.rew_scale_lin_vel_z_penalty, self.cfg.rew_scale_ang_vel_xy_penalty, self.cfg.rew_scale_flat_orientation,
+            self.cfg.rew_scale_unwanted_movement_penalty, self.cfg.rew_scale_dof_at_limit, self.cfg.rew_scale_toe_orientation_penalty,
+            self._toe_joint_indices, self.cfg.rew_scale_low_height_penalty, self.cfg.min_height_penalty_threshold,
             self.cfg.rew_scale_undesired_contact, 
-            self.sim.cfg.dt, # 从 self.sim.cfg.dt 获取
-            # --- 新增参数传递 ---
-            cfg_rew_scale_orientation_deviation=self.cfg.rew_scale_orientation_deviation
+            self.sim.cfg.dt, # 使用 self.sim.cfg.dt
+            getattr(self.cfg, "rew_scale_orientation_deviation", 0.0),
+            self._orientation_termination_angle_limit_rad, # 已在 __init__ 中初始化
+            getattr(self.cfg, "joint_limit_penalty_threshold_percent", 0.05),
+            num_self_collisions_per_env, # 传递自碰撞计数
+            getattr(self.cfg, "rew_scale_self_collision", 0.0), # 传递自碰撞scale
+            # --- 传递翻转奖励相关参数 ---
+            self._was_severely_tilted_last_step,
+            getattr(self.cfg, "rew_scale_successful_flip", 0.0)
         )
+        
+        # 更新上一帧的严重倾斜状态
+        self._was_severely_tilted_last_step = current_is_severely_tilted.clone()
 
+        # ... (日志记录和返回与上一版本相同) ...
         if "log" not in self.extras or self.extras["log"] is None : self.extras["log"] = {}
         for key, value in reward_terms_dict.items():
-            term_mean = value.mean()
-            self.extras["log"][f"reward_term/{key}_step_avg"] = term_mean.item() if torch.is_tensor(term_mean) else term_mean
-            if key not in self._episode_reward_terms_sum:
-                self._episode_reward_terms_sum[key] = torch.zeros(self.num_envs, device=self.device)
+            term_mean = value.mean(); self.extras["log"][f"reward_term/{key}_step_avg"] = term_mean.item() if torch.is_tensor(term_mean) else term_mean
+            if key not in self._episode_reward_terms_sum: self._episode_reward_terms_sum[key] = torch.zeros(self.num_envs, device=self.device)
             self._episode_reward_terms_sum[key] += value.squeeze(-1) if value.ndim > 1 and value.shape[-1] == 1 else value
-
         current_terminated, current_time_out = self._get_dones()
         just_failed_termination = current_terminated & (~current_time_out)
-        final_reward = torch.where(
-            just_failed_termination, torch.full_like(total_reward, self.cfg.rew_scale_termination), total_reward
-        )
+        final_reward = torch.where(just_failed_termination, torch.full_like(total_reward, getattr(self.cfg, "rew_scale_termination", -200.0)), total_reward)
         self.extras["log"]["reward/final_reward_avg"] = final_reward.mean().item() if torch.is_tensor(final_reward) else final_reward.mean()
         return final_reward
 
     def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
         time_out = self.episode_length_buf >= self.max_episode_length - 1
-        
         root_pos_w = self.robot.data.root_pos_w
         projected_gravity_b = self.robot.data.projected_gravity_b
+        cos_angle_robot_z_with_world_z = -projected_gravity_b[:, 2]
+        is_within_termination_orientation_limit = cos_angle_robot_z_with_world_z >= math.cos(self._orientation_termination_angle_limit_rad)
 
-        is_severely_tilted = projected_gravity_b[:, 2] > 0.0
-
+        # 高度过低终止：仅在姿态限制范围内 且 身体相对平坦时 生效
         height_too_low_orig = root_pos_w[:, 2] < self.cfg.termination_height_thresh
-        height_too_low = torch.where(
-            is_severely_tilted, 
-            torch.zeros_like(height_too_low_orig, dtype=torch.bool), # 确保是布尔类型
-            height_too_low_orig
-        )
+        sum_sq_proj_grav_xy = torch.sum(torch.square(projected_gravity_b[:, :2]), dim=1)
+        # flatness_threshold_for_height_termination 从 CFG 读取
+        flatness_thresh = getattr(self.cfg, "flatness_threshold_for_height_termination", 0.2) # 默认 0.2
+        is_relatively_flat = sum_sq_proj_grav_xy < flatness_thresh
+        height_too_low = height_too_low_orig & is_within_termination_orientation_limit & is_relatively_flat
         
         fallen_over_orig = projected_gravity_b[:, 2] > self.cfg.termination_body_z_thresh
-        fallen_over = torch.where(
-            is_severely_tilted,
-            torch.zeros_like(fallen_over_orig, dtype=torch.bool), # 确保是布尔类型
-            fallen_over_orig
-        )
-
+        fallen_over = fallen_over_orig & is_within_termination_orientation_limit
+        
         base_contact_termination = torch.zeros_like(time_out, dtype=torch.bool)
-        if self.cfg.termination_base_contact and self._base_body_id and len(self._base_body_id) > 0:
-             if hasattr(self._contact_sensor.data, 'net_forces_w_history') and self._contact_sensor.data.net_forces_w_history is not None:
-                all_forces_history = self._contact_sensor.data.net_forces_w_history
-                if all_forces_history.ndim == 4 and all_forces_history.shape[1] > 0 and \
-                   self._base_body_id and max(self._base_body_id) < all_forces_history.shape[2]: # 安全检查
-                    current_net_forces_w = all_forces_history[:, -1, :, :]
-                    forces_on_base = current_net_forces_w[:, self._base_body_id, :]
-                    force_magnitudes_base = torch.norm(forces_on_base, dim=-1)
-                    base_contact_termination = torch.any(force_magnitudes_base > 1.0, dim=1)
+        if hasattr(self.cfg, "termination_base_contact") and self.cfg.termination_base_contact:
+            base_contact_raw_detection = torch.zeros_like(time_out, dtype=torch.bool)
+            if self._base_body_id and len(self._base_body_id) > 0:
+                if hasattr(self._contact_sensor.data, 'net_forces_w_history') and \
+                   self._contact_sensor.data.net_forces_w_history is not None:
+                    all_forces = self._contact_sensor.data.net_forces_w_history
+                    if all_forces.ndim == 4 and all_forces.shape[1] > 0 and \
+                       self._base_body_id and max(self._base_body_id) < all_forces.shape[2]:
+                        forces = all_forces[:, -1, self._base_body_id, :]
+                        base_contact_raw_detection = torch.any(torch.norm(forces, dim=-1) > 1.0, dim=1)
+            
+            current_time_in_episode = (self.episode_length_buf + 1).float() * self.physics_dt
+            grace_period_seconds = 1.0
+            ignore_initial_fall_contact = current_time_in_episode <= grace_period_seconds
+            valid_base_contact_trigger = base_contact_raw_detection & (~ignore_initial_fall_contact)
+            base_contact_termination = valid_base_contact_trigger & is_within_termination_orientation_limit
+            
+        terminated = height_too_low | fallen_over | base_contact_termination | time_out
+
+        # # --- (可选) 打印终止原因的逻辑 (与上一版本相同) ---
+        # if torch.is_tensor(terminated) and terminated.any():
+        #     terminated_env_indices = terminated.nonzero(as_tuple=False).squeeze(-1)
+        #     if terminated_env_indices.ndim == 0: terminated_env_indices = terminated_env_indices.unsqueeze(0)
+        #     for env_idx_tensor in terminated_env_indices:
+        #         env_idx = env_idx_tensor.item(); reasons = []
+        #         if height_too_low[env_idx]: reasons.append("height_too_low")
+        #         if fallen_over[env_idx]: reasons.append("fallen_over")
+        #         if base_contact_termination[env_idx]: reasons.append("base_contact")
+        #         if time_out[env_idx]: reasons.append("time_out")
+        #         if reasons:
+        #             episode_step = self.episode_length_buf[env_idx].item() + 1
+        #             print(f"[Termination Info] Env {env_idx}: Reset at ep step {episode_step} due to: {', '.join(reasons)}")
             
-        terminated = height_too_low | fallen_over | base_contact_termination
         return terminated, time_out
     
-    def _reset_idx(self, env_ids: Sequence[int] | None):
+    def _reset_idx(self, env_ids: Sequence[int] | None): # 与上一版本相同 (RPY随机化, num_dof, q_range修正)
         super()._reset_idx(env_ids)
         eids = torch.arange(self.num_envs, device=self.device, dtype=torch.long) if env_ids is None \
             else torch.as_tensor(env_ids, device=self.device, dtype=torch.long)
-        if eids.numel() == 0:
-            return
+        if eids.numel() == 0: return
 
         root_state_reset = self.robot.data.default_root_state[eids].clone()
         if hasattr(self._terrain, 'env_origins') and self._terrain.env_origins is not None:
              root_state_reset[:, :3] += self._terrain.env_origins[eids]
         
         initial_height_base = self.cfg.robot.init_state.pos[2] if self.cfg.robot.init_state.pos is not None and len(self.cfg.robot.init_state.pos) == 3 else 0.3
-        root_state_reset[:, 2] = initial_height_base + self.cfg.reset_height_offset
+        reset_height_offset = getattr(self.cfg, "reset_height_offset", 0.0) # 从CFG获取
+        root_state_reset[:, 2] = initial_height_base + reset_height_offset
 
-        # --- Full RPY Randomization ---
         num_resets = len(eids)
         random_rolls = (torch.rand(num_resets, device=self.device) - 0.5) * 2.0 * math.pi
         random_pitches = (torch.rand(num_resets, device=self.device) - 0.5) * 2.0 * math.pi
         random_yaws = (torch.rand(num_resets, device=self.device) - 0.5) * 2.0 * math.pi
-
         quats_xyzw = torch.zeros(num_resets, 4, device=self.device)
         for i in range(num_resets):
             roll, pitch, yaw = random_rolls[i], random_pitches[i], random_yaws[i]
-            cy = torch.cos(yaw * 0.5)
-            sy = torch.sin(yaw * 0.5)
-            cp = torch.cos(pitch * 0.5)
-            sp = torch.sin(pitch * 0.5)
-            cr = torch.cos(roll * 0.5)
-            sr = torch.sin(roll * 0.5)
-            # Standard ZYX Euler to Quaternion conversion (qx, qy, qz, qw)
-            qw = cr * cp * cy + sr * sp * sy
-            qx = sr * cp * cy - cr * sp * sy
-            qy = cr * sp * cy + sr * cp * sy
-            qz = cr * cp * sy - sr * sp * cy
-            quats_xyzw[i, 0] = qx
-            quats_xyzw[i, 1] = qy
-            quats_xyzw[i, 2] = qz
-            quats_xyzw[i, 3] = qw 
-        root_state_reset[:, 3:7] = convert_quat(quats_xyzw, to="wxyz") # Isaac Sim uses wxyz for root state
-        # --- End RPY Randomization ---
-        
+            cy, sy = torch.cos(yaw * 0.5), torch.sin(yaw * 0.5)
+            cp, sp = torch.cos(pitch * 0.5), torch.sin(pitch * 0.5)
+            cr, sr = torch.cos(roll * 0.5), torch.sin(roll * 0.5)
+            qw = cr * cp * cy + sr * sp * sy; qx = sr * cp * cy - cr * sp * sy
+            qy = cr * sp * cy + sr * cp * sy; qz = cr * cp * sy - sr * sp * cy
+            quats_xyzw[i, 0], quats_xyzw[i, 1], quats_xyzw[i, 2], quats_xyzw[i, 3] = qx, qy, qz, qw
+        root_state_reset[:, 3:7] = convert_quat(quats_xyzw, to="wxyz")
         root_state_reset[:, 7:] = 0.0
         self.robot.write_root_state_to_sim(root_state_reset, eids)
         
-        # --- Joint state reset (in joint limits fully random) ---
-        num_dof = self._q_lower_limits.numel() # CORRECTED
-        
+        num_dof = self._q_lower_limits.numel()
         random_proportions = torch.rand(len(eids), num_dof, device=self.device)
         q_lower_expanded = self._q_lower_limits.unsqueeze(0)
-        q_upper_expanded = self._q_upper_limits.unsqueeze(0)
-        q_range = q_upper_expanded - q_lower_expanded
+        q_range = self._q_upper_limits.unsqueeze(0) - q_lower_expanded
         joint_pos_reset = q_lower_expanded + random_proportions * q_range
-        
         zero_joint_vel = torch.zeros_like(joint_pos_reset)
         self.robot.write_joint_state_to_sim(joint_pos_reset, zero_joint_vel, env_ids=eids)
         self.robot.set_joint_position_target(joint_pos_reset, env_ids=eids)
 
+        # 重置 _was_severely_tilted_last_step
+        # 简单设为True，避免在重置到一个好姿态时立即获得翻转奖励
+        # 更精确的方法是根据重置后的姿态计算初始值，但这可能需要在第一次step/obs之后
+        if hasattr(self, "_was_severely_tilted_last_step"):
+             self._was_severely_tilted_last_step[eids] = True 
+
         cmd_profile = self.cfg.command_profile
-        cmd_duration_str = str(cmd_profile.get("command_mode_duration_s", "20.0"))
-        if cmd_duration_str == "episode_length_s":
-            cmd_duration = self.cfg.episode_length_s
+        cmd_duration_s_config = cmd_profile.get("command_mode_duration_s", self.cfg.episode_length_s if hasattr(self.cfg, "episode_length_s") else 20.0)
+        if isinstance(cmd_duration_s_config, str) and cmd_duration_s_config == "episode_length_s": cmd_duration = self.cfg.episode_length_s
         else:
-            cmd_duration = float(cmd_duration_str)
-            
+            try: cmd_duration = float(cmd_duration_s_config)
+            except (ValueError, TypeError): cmd_duration = self.cfg.episode_length_s if hasattr(self.cfg, "episode_length_s") else 20.0
         self._time_since_last_command_change[eids] = cmd_duration
         self._update_commands(eids)
-
         if hasattr(self, '_previous_policy_actions'): self._previous_policy_actions[eids] = 0.0
         if hasattr(self, '_policy_actions'): self._policy_actions[eids] = 0.0
         for key in list(self._episode_reward_terms_sum.keys()):
             if self._episode_reward_terms_sum[key].shape[0] == self.num_envs :
-                self._episode_reward_terms_sum[key][eids] = 0.0
\ No newline at end of file
+                self._episode_reward_terms_sum[key][eids] = 0.0
diff --git a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py
index d2dfc32..1dbe644 100644
--- a/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py
+++ b/sixfeet_src/sixfeet/source/sixfeet/sixfeet/tasks/direct/sixfeet/sixfeet_env_cfg.py
@@ -79,7 +79,7 @@ class SixfeetEnvCfg(DirectRLEnvCfg):
             ),
         ),
         init_state=ArticulationCfg.InitialStateCfg(
-            pos=(0.0, 0.0, 0.15), # 初始高度较低，便于学习站起
+            pos=(0.0, 0.0, 0.1), # 初始高度较低，便于学习站起
             rot=(1.0, 0.0, 0.0, 0.0), # 标准初始姿态 (w,x,y,z)
             # joint_pos 已被注释掉，将在 env.py 的 _reset_idx 中实现完全随机关节初始姿态
             joint_vel={ # 初始关节速度为0
@@ -131,51 +131,45 @@ class SixfeetEnvCfg(DirectRLEnvCfg):
 
     # -------- 奖励缩放因子 --------
     action_scale: float = 0.5
-
-    # --- 主要的正向激励 (站立) ---
     rew_scale_move_in_commanded_direction: float = 0.0
     rew_scale_achieve_reference_angular_rate: float = 0.0
     rew_scale_alive: float = +0.1
-    rew_scale_target_height: float = +8.0
-    target_height_m: float = 0.20
-
-    # --- 新增：大的姿态偏差惩罚 (用于学习翻转) ---
-    # 这个惩罚项会因为机器人Z轴与世界Z轴的夹角增大而增大 (0到pi)
-    # scale 应该是负值
-    rew_scale_orientation_deviation: float = -50.0  # <--- 新增惩罚项，负值，值越大惩罚越重
+    rew_scale_target_height: float = +20.0 # 将在 env.py 中条件化
+    target_height_m: float = 0.23        # 你的 "Focus on Standing Up" cfg 中 target_height_m 为 0.20，这里用了你新cfg中的0.23
 
-    # --- 行为平滑与效率相关的惩罚 ---
+    rew_scale_orientation_deviation: float = -50.0
+     # --- 新增：自碰撞惩罚 ---
+    rew_scale_self_collision: float = -30.0  # 一个较大的负值，当发生自碰撞时施加
+    rew_scale_successful_flip: float = 40.0 # 当从Z轴朝下翻转到Z轴朝上时给予
     rew_scale_action_cost: float = -0.0001
     rew_scale_action_rate: float = -0.01
     rew_scale_joint_torques: float = -1.0e-6
     rew_scale_joint_accel: float = -1.0e-7
-
-    # --- 姿态与运动稳定性相关的惩罚 ---
     rew_scale_lin_vel_z_penalty: float = -3.0
     rew_scale_ang_vel_xy_penalty: float = -0.1
-    rew_scale_flat_orientation: float = -25.0 # 这个是惩罚小的倾斜（身体不水平），与新的 Z 轴偏差惩罚目标不同
+    rew_scale_flat_orientation: float = -25.0
     rew_scale_unwanted_movement_penalty: float = -5.0
-
-    # --- 行为约束相关的惩罚 ---
     rew_scale_dof_at_limit: float = -0.5
-    rew_scale_toe_orientation_penalty: float = -4.0 # 条件化生效
+    rew_scale_toe_orientation_penalty: float = -4.0 # 在 env.py 中条件化生效
     rew_scale_low_height_penalty: float = -30.0
-    min_height_penalty_threshold: float = 0.12
-
-    # --- 不期望的接触惩罚 ---
-    rew_scale_undesired_contact: float = -5.0   # 条件化生效
+    min_height_penalty_threshold: float = 0.12 # 你的 "Focus on Standing Up" cfg 中是 0.15
 
-    # --- 终止状态相关的惩罚 ---
+    rew_scale_undesired_contact: float = -5.0   # 在 env.py 中条件化生效
     rew_scale_termination: float = -20.0
 
-    # --- 新增：用于条件化终止的方向角度限制 ---
-    # 这些终止条件仅在机器人Z轴与世界Z轴的夹角在此限制内时生效
-    orientation_termination_angle_limit_deg: float = 90.0 # 度, +/- 此值范围
+    # --- 新增：可配置的关节极限惩罚阈值 (百分比) ---
+    joint_limit_penalty_threshold_percent: float = 0.05  # 例如 0.05 代表边缘5%
 
-    # ... (Randomisation 和 Termination 条件保持不变, 但其生效逻辑会在 env.py 中被条件化) ...
+    # -------- 重置随机化 --------
     root_orientation_yaw_range: float = math.pi
     reset_height_offset: float = 0.0
+
+    # -------- 终止条件 --------
     termination_body_z_thresh: float = 0.95
     termination_height_thresh: float = 0.05
-    termination_base_contact: bool = False
-    
\ No newline at end of file
+    termination_base_contact: bool = False # 已设为 True
+    
+    # --- 用于条件化终止的姿态角度限制 (度) ---
+    orientation_termination_angle_limit_deg: float = 95.0
+     # 值越小，表示要求机器人越平坦。例如 0.1*0.1 = 0.01 (约5.7度倾斜)，0.3*0.3 = 0.09 (约17度倾斜)
+    flatness_threshold_for_height_termination: float = 0.02 # 可调整
\ No newline at end of file